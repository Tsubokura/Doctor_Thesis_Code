{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01dc158-d974-4961-a1d9-aadb67790488",
   "metadata": {},
   "outputs": [],
   "source": [
    "####データセット全体にて標準化し，保存し直す\n",
    "\n",
    "\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# ===================== 設定 =====================\n",
    "\n",
    "# 元データセット（クラスディレクトリ配下に .npy）\n",
    "DATASET_ROOT = Path(\"./All_materials/\")\n",
    "\n",
    "# 出力先（ここ配下に timestamp/クラス名/*.npy を作る）\n",
    "OUTPUT_ROOT = Path(\"./normalized_dataset_forplot\")\n",
    "\n",
    "# オフセット範囲（両端含む）\n",
    "OFFSET_T0 = 0\n",
    "OFFSET_T1 = 100\n",
    "\n",
    "# 数値安定化\n",
    "EPS = 1e-8\n",
    "\n",
    "# 保存 dtype（float32 推奨）\n",
    "SAVE_DTYPE = np.float32\n",
    "\n",
    "# 既に出力がある場合にスキップするか\n",
    "SKIP_IF_EXISTS = True\n",
    "\n",
    "\n",
    "# ===================== ユーティリティ =====================\n",
    "\n",
    "def list_classes(dataset_root: Path):\n",
    "    classes = [d.name for d in dataset_root.iterdir() if d.is_dir()]\n",
    "    classes.sort()\n",
    "    return classes\n",
    "\n",
    "def list_npy_files(class_dir: Path):\n",
    "    files = [p for p in class_dir.glob(\"*.npy\")]\n",
    "    files.sort()\n",
    "    return files\n",
    "\n",
    "def iter_all_npy_files(dataset_root: Path):\n",
    "    \"\"\"(class_name, path) を全て列挙\"\"\"\n",
    "    for cls in list_classes(dataset_root):\n",
    "        class_dir = dataset_root / cls\n",
    "        for p in list_npy_files(class_dir):\n",
    "            yield cls, p\n",
    "\n",
    "\n",
    "# ===================== 統計量（global mean/std）の推定 =====================\n",
    "\n",
    "def compute_global_stats_with_offset(dataset_root: Path,\n",
    "                                     offset_t0: int = 0,\n",
    "                                     offset_t1: int = 100,\n",
    "                                     eps: float = 1e-8):\n",
    "    \"\"\"\n",
    "    全 .npy を対象に:\n",
    "      1) 各 trial で t=offset_t0..offset_t1 の平均を引く（taxel×軸ごと）\n",
    "      2) オフセット除去後の global mean/std を taxel×軸ごとに推定\n",
    "\n",
    "    Returns:\n",
    "      mean: (n_taxels, 3)\n",
    "      std : (n_taxels, 3)\n",
    "    \"\"\"\n",
    "    paths = list(iter_all_npy_files(dataset_root))\n",
    "    if not paths:\n",
    "        raise RuntimeError(f\"No .npy files found under: {dataset_root}\")\n",
    "\n",
    "    sum_ = None\n",
    "    sumsq_ = None\n",
    "    total_count = 0\n",
    "    n_taxels_ref = None\n",
    "\n",
    "    for i, (_, path) in enumerate(paths):\n",
    "        data = np.load(path)  # (T, n_taxels, 3)\n",
    "\n",
    "        if data.ndim != 3 or data.shape[-1] != 3:\n",
    "            raise ValueError(f\"Invalid shape {data.shape} in {path} (expected (T, n_taxels, 3))\")\n",
    "\n",
    "        T, n_taxels, _ = data.shape\n",
    "        if T <= offset_t1:\n",
    "            raise ValueError(f\"T={T} too short for offset range {offset_t0}..{offset_t1} in {path}\")\n",
    "\n",
    "        if n_taxels_ref is None:\n",
    "            n_taxels_ref = n_taxels\n",
    "            sum_ = np.zeros((n_taxels_ref, 3), dtype=np.float64)\n",
    "            sumsq_ = np.zeros((n_taxels_ref, 3), dtype=np.float64)\n",
    "        elif n_taxels != n_taxels_ref:\n",
    "            raise ValueError(f\"n_taxels mismatch: {n_taxels} != {n_taxels_ref} in {path}\")\n",
    "\n",
    "        baseline = data[offset_t0:offset_t1 + 1].mean(axis=0, keepdims=True)  # (1, n_taxels, 3)\n",
    "        data0 = (data - baseline).astype(np.float64)  # (T, n_taxels, 3)\n",
    "\n",
    "        sum_ += data0.sum(axis=0)             # (n_taxels, 3)\n",
    "        sumsq_ += (data0 * data0).sum(axis=0) # (n_taxels, 3)\n",
    "        total_count += T\n",
    "\n",
    "        if (i + 1) % 200 == 0:\n",
    "            print(f\"[global-stats] processed {i+1}/{len(paths)} files...\")\n",
    "\n",
    "    mean = sum_ / total_count\n",
    "    var = (sumsq_ / total_count) - (mean * mean)\n",
    "    var = np.maximum(var, 0.0)\n",
    "    std = np.sqrt(var + eps)\n",
    "\n",
    "    return mean.astype(np.float32), std.astype(np.float32)\n",
    "\n",
    "\n",
    "def normalize_trial(data: np.ndarray, mean: np.ndarray, std: np.ndarray,\n",
    "                    offset_t0: int = 0, offset_t1: int = 100):\n",
    "    \"\"\"\n",
    "    data: (T, n_taxels, 3)\n",
    "    mean/std: (n_taxels, 3) （オフセット後のglobal統計）\n",
    "    \"\"\"\n",
    "    T, n_taxels, _ = data.shape\n",
    "    if T <= offset_t1:\n",
    "        raise ValueError(f\"T={T} too short for offset range {offset_t0}..{offset_t1}\")\n",
    "\n",
    "    baseline = data[offset_t0:offset_t1 + 1].mean(axis=0, keepdims=True)  # (1, n_taxels, 3)\n",
    "    data0 = data - baseline\n",
    "    data_norm = (data0 - mean[None, :, :]) / std[None, :, :]\n",
    "    return data_norm\n",
    "\n",
    "\n",
    "# ===================== 保存処理（同ディレクトリ構造） =====================\n",
    "\n",
    "def save_normalized_dataset(dataset_root: Path,\n",
    "                            out_root: Path,\n",
    "                            mean: np.ndarray,\n",
    "                            std: np.ndarray,\n",
    "                            offset_t0: int = 0,\n",
    "                            offset_t1: int = 100,\n",
    "                            save_dtype=np.float32,\n",
    "                            skip_if_exists: bool = True):\n",
    "    \"\"\"\n",
    "    DATASET_ROOT の構造:\n",
    "      dataset_root/\n",
    "        classA/*.npy\n",
    "        classB/*.npy\n",
    "        ...\n",
    "\n",
    "    OUTPUT の構造:\n",
    "      out_root/\n",
    "        classA/*.npy\n",
    "        classB/*.npy\n",
    "        ...\n",
    "\n",
    "    を維持して保存する。\n",
    "    \"\"\"\n",
    "    all_items = list(iter_all_npy_files(dataset_root))\n",
    "    print(f\"[save] total files to process: {len(all_items)}\")\n",
    "\n",
    "    for i, (cls, path) in enumerate(all_items):\n",
    "        rel_class_dir = cls\n",
    "        out_class_dir = out_root / rel_class_dir\n",
    "        out_class_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        out_path = out_class_dir / path.name\n",
    "        if skip_if_exists and out_path.exists():\n",
    "            continue\n",
    "\n",
    "        data = np.load(path)  # (T, n_taxels, 3)\n",
    "        data_norm = normalize_trial(data, mean, std, offset_t0, offset_t1).astype(save_dtype)\n",
    "\n",
    "        # 念のため shape チェック\n",
    "        if data_norm.shape != data.shape:\n",
    "            raise RuntimeError(f\"shape changed: {data.shape} -> {data_norm.shape} at {path}\")\n",
    "\n",
    "        np.save(out_path, data_norm)\n",
    "\n",
    "        if (i + 1) % 200 == 0:\n",
    "            print(f\"[save] processed {i+1}/{len(all_items)} files...\")\n",
    "\n",
    "    print(\"[save] done.\")\n",
    "\n",
    "\n",
    "# ===================== main =====================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # timestamp 付きの出力ディレクトリを作る\n",
    "    timestamp_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    RUN_OUT_DIR = OUTPUT_ROOT / timestamp_str\n",
    "    RUN_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"DATASET_ROOT : {DATASET_ROOT}\")\n",
    "    print(f\"OUTPUT_DIR   : {RUN_OUT_DIR}\")\n",
    "    print(f\"OFFSET range : {OFFSET_T0}..{OFFSET_T1} (inclusive)\")\n",
    "\n",
    "    # 1) global mean/std を計算（オフセット後）\n",
    "    print(\"\\n[1/2] computing global mean/std ...\")\n",
    "    global_mean, global_std = compute_global_stats_with_offset(\n",
    "        DATASET_ROOT, offset_t0=OFFSET_T0, offset_t1=OFFSET_T1, eps=EPS\n",
    "    )\n",
    "    print(\"[1/2] done.\")\n",
    "    print(\"  mean shape:\", global_mean.shape, \"std shape:\", global_std.shape)\n",
    "    print(\"  std  min/max:\", float(global_std.min()), float(global_std.max()))\n",
    "\n",
    "    # 統計量と設定を保存（再現用）\n",
    "    np.save(RUN_OUT_DIR / \"global_mean_taxel_axis.npy\", global_mean)\n",
    "    np.save(RUN_OUT_DIR / \"global_std_taxel_axis.npy\", global_std)\n",
    "    with open(RUN_OUT_DIR / \"normalization_info.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"DATASET_ROOT={DATASET_ROOT}\\n\")\n",
    "        f.write(f\"OFFSET_T0={OFFSET_T0}\\n\")\n",
    "        f.write(f\"OFFSET_T1={OFFSET_T1}\\n\")\n",
    "        f.write(f\"EPS={EPS}\\n\")\n",
    "        f.write(f\"SAVE_DTYPE={save_dtype_to_str(SAVE_DTYPE) if 'save_dtype_to_str' in globals() else str(SAVE_DTYPE)}\\n\")\n",
    "        f.write(f\"SKIP_IF_EXISTS={SKIP_IF_EXISTS}\\n\")\n",
    "\n",
    "    # 2) 全ファイルを正規化して同構造で保存\n",
    "    print(\"\\n[2/2] normalizing and saving dataset ...\")\n",
    "    save_normalized_dataset(\n",
    "        dataset_root=DATASET_ROOT,\n",
    "        out_root=RUN_OUT_DIR,\n",
    "        mean=global_mean,\n",
    "        std=global_std,\n",
    "        offset_t0=OFFSET_T0,\n",
    "        offset_t1=OFFSET_T1,\n",
    "        save_dtype=SAVE_DTYPE,\n",
    "        skip_if_exists=SKIP_IF_EXISTS\n",
    "    )\n",
    "    print(f\"\\nAll done. Normalized dataset saved under: {RUN_OUT_DIR}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
