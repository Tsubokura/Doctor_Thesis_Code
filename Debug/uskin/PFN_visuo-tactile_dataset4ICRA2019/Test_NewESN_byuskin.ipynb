{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1086850-ece8-44f5-91f7-c9d86e4ebbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random \n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from scipy import signal\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, random_split, Subset\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "from esn_model import ESN, ReadOut\n",
    "\n",
    "# ヘルパー\n",
    "from rc_timeseries_helpers import (\n",
    "    infer_device_from_model,\n",
    "    extract_states_time_major,\n",
    "    extract_logits_time_major,\n",
    "    apply_time_selection,\n",
    "    prepare_time_distributed_targets,\n",
    "    compute_loss_time_kept,\n",
    "    sequence_accuracy_majority_vote\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "label_encoder = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62c1fb70-a35e-4f10-a238-dbc09998a69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnekodaisuki169\u001b[0m (\u001b[33mdoctor_thesis_material\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/sota/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "# WandBの設定\n",
    "WANDB_API_KEY = \"2d996a98ef8dddefa91d675f85b5efd96fb911ae\"  # あなたのWandB APIキーをここに入力してください\n",
    "\n",
    "wandb.login(key = WANDB_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a27cf279-2a3a-4c93-a60e-9d2d1c6ff966",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TactileSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    触覚センサ npy データ用 Dataset クラス（CV/Stratify向け）\n",
    "\n",
    "    - 入力ファイル: (T, n_taxels, 3)\n",
    "    - [seq_start:seq_end] を切り出し -> (seq_len, n_taxels*3)\n",
    "    - ESN向けに (1, seq_len, feature_dim) を返す\n",
    "    - ラベルは one-hot ではなく「class index (long)」を返す（rc_timeseries_helpers が one-hot化する）\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, dataset_params):\n",
    "        super().__init__()\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.seq_start = int(dataset_params[\"seq_start\"])\n",
    "        self.seq_end   = int(dataset_params[\"seq_end\"])\n",
    "        self.dtype     = torch.float32\n",
    "        self.mmap_mode = dataset_params.get(\"mmap_mode\", None)  # \"r\" 推奨（必要なら）\n",
    "\n",
    "        # クラスディレクトリ列挙\n",
    "        self.class_names = sorted([d.name for d in self.root_dir.iterdir() if d.is_dir()])\n",
    "        if not self.class_names:\n",
    "            raise RuntimeError(f\"No class directories under {self.root_dir}\")\n",
    "\n",
    "        self.class_to_idx = {c: i for i, c in enumerate(self.class_names)}\n",
    "        self.num_classes  = len(self.class_names)\n",
    "\n",
    "        # samples: (path, label_idx)\n",
    "        self.samples = []\n",
    "        for class_name in self.class_names:\n",
    "            class_dir = self.root_dir / class_name\n",
    "            for npy_path in sorted(class_dir.glob(\"*.npy\")):\n",
    "                self.samples.append((npy_path, self.class_to_idx[class_name]))\n",
    "\n",
    "        if not self.samples:\n",
    "            raise RuntimeError(f\"No .npy files found under {self.root_dir}\")\n",
    "\n",
    "        # ★ これが StratifiedKFold 用に効く\n",
    "        self.labels = [lab for _, lab in self.samples]\n",
    "\n",
    "        # 先頭で形状チェック\n",
    "        arr0 = np.load(self.samples[0][0], mmap_mode=self.mmap_mode)\n",
    "        if arr0.ndim != 3:\n",
    "            raise ValueError(f\"Expected (T, n_taxels, 3), got shape={arr0.shape}\")\n",
    "        T, n_taxels, axes = arr0.shape\n",
    "        if axes != 3:\n",
    "            raise ValueError(f\"Last dim must be 3, got {axes}\")\n",
    "        if self.seq_end > T:\n",
    "            raise ValueError(f\"seq_end({self.seq_end}) > T({T}). Please adjust.\")\n",
    "\n",
    "        self.original_T  = T\n",
    "        self.n_taxels    = n_taxels\n",
    "        self.seq_len     = self.seq_end - self.seq_start\n",
    "        self.feature_dim = self.n_taxels * 3\n",
    "\n",
    "        print(\"=== TactileSequenceDataset initialized ===\")\n",
    "        print(f\"root_dir     : {self.root_dir}\")\n",
    "        print(f\"num_classes  : {self.num_classes}\")\n",
    "        print(f\"num_samples  : {len(self.samples)}\")\n",
    "        print(f\"original T   : {self.original_T}\")\n",
    "        print(f\"seq range    : [{self.seq_start}, {self.seq_end}) -> seq_len={self.seq_len}\")\n",
    "        print(f\"n_taxels     : {self.n_taxels}\")\n",
    "        print(f\"feature_dim  : {self.feature_dim} (= n_taxels * 3)\")\n",
    "        print(\"class_to_idx : \", self.class_to_idx)\n",
    "        print(\"=========================================\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        npy_path, label_idx = self.samples[idx]\n",
    "\n",
    "        arr = np.load(npy_path, mmap_mode=self.mmap_mode)  # (T, n_taxels, 3)\n",
    "        seq = arr[self.seq_start:self.seq_end]             # (seq_len, n_taxels, 3)\n",
    "\n",
    "        # (seq_len, n_taxels*3)\n",
    "        seq = np.asarray(seq, dtype=np.float32).reshape(self.seq_len, self.feature_dim)\n",
    "\n",
    "        # x: (1, seq_len, feature_dim)\n",
    "        x = torch.from_numpy(seq).to(self.dtype).unsqueeze(0)\n",
    "\n",
    "        # y: class index (long)\n",
    "        y = torch.tensor(label_idx, dtype=torch.long)\n",
    "\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "740eec8e-5224-42d6-a6e2-1bd798a2f610",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def _unwrap_subset(ds):\n",
    "    \"\"\"Subset(Subset(...)) でも base dataset と base index 列に展開して返す\"\"\"\n",
    "    idxs = list(range(len(ds)))\n",
    "    while isinstance(ds, Subset):\n",
    "        idxs = [ds.indices[i] for i in idxs]\n",
    "        ds = ds.dataset\n",
    "    return ds, idxs\n",
    "\n",
    "def _get_stratify_labels(ds):\n",
    "    \"\"\"\n",
    "    StratifiedKFold 用 y を返す（dataset.labels を最優先）。\n",
    "    labels が無ければ samples/targets から取る。最終手段は __getitem__ で推定（遅い）。\n",
    "    \"\"\"\n",
    "    base, base_idxs = _unwrap_subset(ds)\n",
    "\n",
    "    if hasattr(base, \"labels\"):\n",
    "        base_labels = list(base.labels)\n",
    "        return [base_labels[i] for i in base_idxs]\n",
    "\n",
    "    if hasattr(base, \"samples\"):\n",
    "        base_labels = [lab for _, lab in base.samples]\n",
    "        return [base_labels[i] for i in base_idxs]\n",
    "\n",
    "    if hasattr(base, \"targets\"):\n",
    "        base_labels = list(base.targets)\n",
    "        return [base_labels[i] for i in base_idxs]\n",
    "\n",
    "    # fallback（遅い）：getitem から推定（y が index/one-hot どちらでもOK）\n",
    "    labels = []\n",
    "    for i in range(len(ds)):\n",
    "        y = ds[i][1]\n",
    "        if torch.is_tensor(y):\n",
    "            if y.dtype.is_floating_point:  # one-hot\n",
    "                labels.append(int(y.argmax().item()))\n",
    "            else:  # index\n",
    "                labels.append(int(y.view(-1)[0].item()))\n",
    "        else:\n",
    "            labels.append(int(y))\n",
    "    return labels\n",
    "\n",
    "def create_cross_validation_dataloaders(dataset, dataset_params, traing_params):\n",
    "    \"\"\"\n",
    "    - dataset が Dataset / Subset / Subsetのネスト いずれでもOK\n",
    "    - dataset.labels を最優先で stratify を作る\n",
    "    - 正規化はしない（Datasetをそのまま読む）\n",
    "    - “フルバッチ” ローダを返す（既存方針維持）\n",
    "    \"\"\"\n",
    "    n_splits   = int(traing_params[\"n_splits\"])\n",
    "    seed       = int(traing_params.get(\"seed\", dataset_params.get(\"seed\", 0)))\n",
    "\n",
    "    y = _get_stratify_labels(dataset)\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "\n",
    "    loaders = []\n",
    "    for tr_idx, va_idx in skf.split(range(len(dataset)), y):\n",
    "        tr = Subset(dataset, tr_idx)\n",
    "        va = Subset(dataset, va_idx)\n",
    "\n",
    "        # 既存の「フルバッチ」方針を維持（batch_size は無視して全件）\n",
    "        loaders.append((\n",
    "            DataLoader(tr, len(tr), shuffle=True),\n",
    "            DataLoader(va, len(va), shuffle=False),\n",
    "        ))\n",
    "\n",
    "    return loaders\n",
    "\n",
    "\n",
    "def prepare_datasets(dataset_params, traing_params, data_dir):\n",
    "    testdata_ratio = float(traing_params[\"testdata_ratio\"])\n",
    "    batch_size = int(dataset_params[\"batch_size\"])\n",
    "    \n",
    "    # データセットの準備\n",
    "    dataset = TactileSequenceDataset(data_dir, dataset_params)  # 拡張はここでは適用しない\n",
    "\n",
    "    # データセットを学習用、テスト用に分割する\n",
    "    test_size = int(len(dataset) * testdata_ratio)\n",
    "    train_size = len(dataset) - test_size\n",
    "    crossval_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    # 交差検証のためのデータローダーを準備\n",
    "    cross_val_loaders = create_cross_validation_dataloaders(crossval_dataset, dataset_params, traing_params)\n",
    "\n",
    "    # テストデータローダーの準備\n",
    "    test_loader = DataLoader(test_dataset, batch_size, shuffle=False)\n",
    "\n",
    "    return cross_val_loaders, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5d2bbce-44c4-42dd-b2ed-f5ebfd93bfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =========================\n",
    "# 2) train_model / validate_model を丸ごと置き換え\n",
    "# =========================\n",
    "def train_model(model, criterion, optimizer, train_loader, dataset_params, model_params, traing_params):\n",
    "    model.train()\n",
    "\n",
    "    num_epochs = int(traing_params[\"num_epochs\"])\n",
    "    batch_training = bool(model_params[\"Batch_Training\"])\n",
    "    Regularization_L2 = float(model_params[\"Regularization_L2\"])\n",
    "\n",
    "    device = infer_device_from_model(model, fallback=\"cpu\")\n",
    "\n",
    "    washout_steps = int(dataset_params.get(\"washout_steps\", 0))\n",
    "    time_stride   = int(dataset_params.get(\"time_stride\", 1))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        loss_sum = 0.0\n",
    "        acc_sum  = 0.0\n",
    "        n_seen   = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.float().to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # ---- forward ----\n",
    "            esn_out = model.ESN(inputs)         # [B,1,T,H] or [B,T,H]\n",
    "            ro_out  = model.ReadOut(esn_out)    # [B,1,T,C] or [B,T,C]\n",
    "\n",
    "            # ---- normalize shapes to time-major ----\n",
    "            states_BTH = extract_states_time_major(esn_out)  # [B,T,H]\n",
    "            logits_BTC = extract_logits_time_major(ro_out)   # [B,T,C]\n",
    "\n",
    "            if states_BTH.shape[1] != logits_BTC.shape[1]:\n",
    "                raise ValueError(f\"T mismatch: states T={states_BTH.shape[1]}, logits T={logits_BTC.shape[1]}\")\n",
    "\n",
    "            # ---- washout / stride ----\n",
    "            states_BTH = apply_time_selection(states_BTH, washout_steps=washout_steps, time_stride=time_stride)\n",
    "            logits_BTC = apply_time_selection(logits_BTC, washout_steps=washout_steps, time_stride=time_stride)\n",
    "\n",
    "            B, T_eff, H = states_BTH.shape\n",
    "            _, _, C = logits_BTC.shape\n",
    "\n",
    "            # ---- loss (T kept) + targets for ridge ----\n",
    "            loss, targets_onehot_BTC, target_index_B = compute_loss_time_kept(\n",
    "                logits_time_major=logits_BTC,\n",
    "                labels=labels,\n",
    "                criterion=criterion\n",
    "            )\n",
    "\n",
    "            # ---- update ----\n",
    "            if batch_training:\n",
    "                # ridge update requires 2D: X [H, B*T], Y [C, B*T]\n",
    "                X = states_BTH.permute(2, 0, 1).contiguous().view(H, -1)                  # [H, B*T]\n",
    "                Y = targets_onehot_BTC.permute(2, 0, 1).contiguous().view(C, -1)          # [C, B*T]\n",
    "                model.ReadOut.ridge_regression_update(X, Y, model, Regularization_L2)\n",
    "            else:\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # ---- metrics (sequence-level) ----\n",
    "            acc = sequence_accuracy_majority_vote(logits_BTC, target_index_B)\n",
    "\n",
    "            loss_sum += float(loss.item()) * B\n",
    "            acc_sum  += float(acc) * B\n",
    "            n_seen   += B\n",
    "\n",
    "        epoch_loss = loss_sum / max(n_seen, 1)\n",
    "        epoch_acc  = acc_sum  / max(n_seen, 1)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {epoch_loss:.4f} | Acc: {epoch_acc*100:.2f}%\")\n",
    "        try:\n",
    "            wandb.log({\"loss\": epoch_loss, \"train_accuracy\": epoch_acc})\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    print(\"Training complete\")\n",
    "\n",
    "\n",
    "def validate_model(model, val_loader, dataset_params, model_params, criterion):\n",
    "    model.eval()\n",
    "\n",
    "    device = infer_device_from_model(model, fallback=\"cpu\")\n",
    "    washout_steps = int(dataset_params.get(\"washout_steps\", 0))\n",
    "    time_stride   = int(dataset_params.get(\"time_stride\", 1))\n",
    "\n",
    "    loss_sum = 0.0\n",
    "    acc_sum  = 0.0\n",
    "    n_seen   = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.float().to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            esn_out = model.ESN(inputs)\n",
    "            ro_out  = model.ReadOut(esn_out)\n",
    "\n",
    "            states_BTH = extract_states_time_major(esn_out)\n",
    "            logits_BTC = extract_logits_time_major(ro_out)\n",
    "\n",
    "            if states_BTH.shape[1] != logits_BTC.shape[1]:\n",
    "                raise ValueError(f\"T mismatch: states T={states_BTH.shape[1]}, logits T={logits_BTC.shape[1]}\")\n",
    "\n",
    "            states_BTH = apply_time_selection(states_BTH, washout_steps=washout_steps, time_stride=time_stride)\n",
    "            logits_BTC = apply_time_selection(logits_BTC, washout_steps=washout_steps, time_stride=time_stride)\n",
    "\n",
    "            B = logits_BTC.shape[0]\n",
    "\n",
    "            loss, _, target_index_B = compute_loss_time_kept(\n",
    "                logits_time_major=logits_BTC,\n",
    "                labels=labels,\n",
    "                criterion=criterion\n",
    "            )\n",
    "            acc = sequence_accuracy_majority_vote(logits_BTC, target_index_B)\n",
    "\n",
    "            loss_sum += float(loss.item()) * B\n",
    "            acc_sum  += float(acc) * B\n",
    "            n_seen   += B\n",
    "\n",
    "    val_loss = loss_sum / max(n_seen, 1)\n",
    "    val_acc  = acc_sum  / max(n_seen, 1)\n",
    "\n",
    "    print(f\"Validation | Loss: {val_loss:.4f} | Acc: {val_acc*100:.2f}%\")\n",
    "    try:\n",
    "        wandb.log({\"val_loss\": val_loss, \"val_accuracy\": val_acc})\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return val_loss, val_acc\n",
    "\n",
    "\n",
    " \n",
    "    \n",
    "def test_model(model, val_loader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_running_loss = 0.0\n",
    "    val_running_corrects = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.float().to(device)\n",
    "            labels = labels.squeeze().to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "#             _, label_preds = torch.max(labels, 1)\n",
    "            label_preds = labels\n",
    "\n",
    "            val_running_loss += loss.item() * inputs.size(0)\n",
    "            print(loss.item())\n",
    "            val_running_corrects += torch.sum(preds == label_preds)\n",
    "\n",
    "    val_loss = val_running_loss / len(val_loader.dataset)\n",
    "    val_accuracy = val_running_corrects.double() / len(val_loader.dataset)\n",
    "    \n",
    "    print(f'Test Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}')\n",
    "    wandb.log({\"test_accuracy\": val_accuracy, \"test_loss\": val_loss})\n",
    "\n",
    "def compute_accuracy(model_output, target, n_taus):\n",
    "    # モデルの出力と教師ラベルを各データに分割\n",
    "    split_model_output = torch.split(model_output.squeeze(), n_taus, dim=-1)\n",
    "    split_target = torch.split(target.squeeze(), n_taus, dim=-1)\n",
    "    # print(\"aaaa\")\n",
    "    # print(split_model_output[0].shape)\n",
    "    # print(split_target[0].shape)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for pred, true_label in zip(split_model_output, split_target):\n",
    "        # 最も確率が高いラベルを予測ラベルとして取得\n",
    "        # print(pred.shape)\n",
    "        # print(pred)\n",
    "        # print(true_label.shape)\n",
    "        # print(true_label)\n",
    "        count_ones = (true_label == 1).sum().item()\n",
    "        # print(count_ones)\n",
    "        histgram_predict = torch.bincount(torch.max(pred, 0)[1])\n",
    "        _, predicted = torch.max(histgram_predict, 0)\n",
    "    \n",
    "        histgram_true_label_idx = torch.bincount(torch.max(true_label, 0)[1])\n",
    "        _, true_label_idx = torch.max(histgram_true_label_idx, 0)\n",
    "\n",
    "        # 正解数をカウント\n",
    "        correct += (predicted == true_label_idx).sum().item()\n",
    "        total += 1\n",
    "\n",
    "    # 精度を算出\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "def model_params_candinate(model_params):\n",
    "    model_params_combinations = list(itertools.product(*model_params.values()))\n",
    "    param_dicts = [dict(zip(model_params.keys(), combination)) for combination in model_params_combinations]\n",
    "    return param_dicts\n",
    "\n",
    "# モデル構造を辞書型に格納\n",
    "def model_sturcture_dict(model):\n",
    "    layers_dict = {}\n",
    "    for name, module in model.named_modules():\n",
    "        layers_dict[name] = {\n",
    "            'type': type(module).__name__,\n",
    "            'parameters': {p: getattr(module, p) for p in module.__dict__ if not p.startswith('_')}\n",
    "        }\n",
    "    # モデル名と初期の引数は削除\n",
    "    del(layers_dict[''])\n",
    "    return layers_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "071972aa-1c27-42aa-b8aa-3e8deaa640c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold=0 overlap_paths=0\n",
      "fold=1 overlap_paths=0\n",
      "fold=2 overlap_paths=0\n",
      "fold=3 overlap_paths=0\n",
      "fold=4 overlap_paths=0\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "def base_indices(ds):\n",
    "    idxs = list(range(len(ds)))\n",
    "    while isinstance(ds, Subset):\n",
    "        idxs = [ds.indices[i] for i in idxs]\n",
    "        ds = ds.dataset\n",
    "    return ds, idxs\n",
    "\n",
    "def paths_from_loader(loader):\n",
    "    base, idxs = base_indices(loader.dataset)\n",
    "    return set(str(base.samples[i][0]) for i in idxs)\n",
    "\n",
    "for fold, (tr, va) in enumerate(cross_val_loaders):\n",
    "    inter = paths_from_loader(tr) & paths_from_loader(va)\n",
    "    print(f\"fold={fold} overlap_paths={len(inter)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfea50b7-f8b1-4f90-af54-c210629f34b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TactileSequenceDataset initialized ===\n",
      "root_dir     : normalized_dataset/20251215_161803\n",
      "num_classes  : 25\n",
      "num_samples  : 250\n",
      "original T   : 1255\n",
      "seq range    : [400, 1200) -> seq_len=800\n",
      "n_taxels     : 16\n",
      "feature_dim  : 48 (= n_taxels * 3)\n",
      "class_to_idx :  {'01_table_cover': 0, '02_fur_scarf': 1, '03_washing_towel': 2, '04_carpet1': 3, '05_bubble_wrap': 4, '06_fleece_scarf': 5, '07_knit_hat1': 6, '08_body_towel1': 7, '09_body_towel2': 8, '10_carpet2': 9, '11_work_gloves': 10, '12_knit_hat2': 11, '13_toilet_mat1': 12, '14_floor_mat': 13, '15_sponge1': 14, '16_printed_tatami': 15, '17_cushion1': 16, '18_mop': 17, '19_toilet_mat2': 18, '20_fleece_sock': 19, '21_cushion': 20, '22_carpet3': 21, '23_fleece_mat': 22, '24_carpet4': 23, '25_sponge2': 24}\n",
      "=========================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:7wzr9y0l) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B sync reduced upload amount by 9.1%             "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>▁</td></tr><tr><td>train_accuracy</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>1.42836</td></tr><tr><td>train_accuracy</td><td>0.05</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">resilient-shape-24</strong> at: <a href='https://wandb.ai/doctor_thesis_material/uskin_test_NewESN/runs/7wzr9y0l' target=\"_blank\">https://wandb.ai/doctor_thesis_material/uskin_test_NewESN/runs/7wzr9y0l</a><br/> View project at: <a href='https://wandb.ai/doctor_thesis_material/uskin_test_NewESN' target=\"_blank\">https://wandb.ai/doctor_thesis_material/uskin_test_NewESN</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251223_115921-7wzr9y0l/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:7wzr9y0l). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acd0e4c80976475db7a395807e3ce6eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011143246755556094, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/sota/Doctor_Thesis_Code/Debug/uskin/PFN_visuo-tactile_dataset4ICRA2019/wandb/run-20251223_115953-x1ilkfvx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/doctor_thesis_material/uskin_test_NewESN/runs/x1ilkfvx' target=\"_blank\">trim-wood-25</a></strong> to <a href='https://wandb.ai/doctor_thesis_material/uskin_test_NewESN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/doctor_thesis_material/uskin_test_NewESN' target=\"_blank\">https://wandb.ai/doctor_thesis_material/uskin_test_NewESN</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/doctor_thesis_material/uskin_test_NewESN/runs/x1ilkfvx' target=\"_blank\">https://wandb.ai/doctor_thesis_material/uskin_test_NewESN/runs/x1ilkfvx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m model \u001b[38;5;241m=\u001b[39m ESN(each_model_params, training_params, dataset_params)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     43\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[0;32m---> 44\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meach_model_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m validate_model(model, val_loader, dataset_params, each_model_params, criterion)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# model.__init__(each_model_params, training_params, dataset_params)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 23\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, train_loader, dataset_params, model_params, traing_params)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     22\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 23\u001b[0m     B, C \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     24\u001b[0m     rand_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, C, (B,), device\u001b[38;5;241m=\u001b[39mlabels\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     25\u001b[0m     labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mone_hot(rand_idx, num_classes\u001b[38;5;241m=\u001b[39mC)\u001b[38;5;241m.\u001b[39mfloat()\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#各種のパラメータ設定\n",
    "dataset_params = {\"seq_start\" : 400, \"seq_end\" : 1200, \"sequence_length\": 800, \"slicing_size\" : 1, \"augmentation_factor\": 0, \"batch_size\" : 32, \"Onehot_Encoding\" : None, \"augmentation_mu\" : 0, \"augmentation_sigma\" : 0, \"augmentation_shift\" : 1, \"time_stride\":1}\n",
    "model_params = {\"reservoir_size\" : [100],\"input_size\" : [48], \"channel_size\" : [1],  \"reservoir_weights_scale\" : [1], \"input_weights_scale\" : [10000000], \"spectral_radius\" : [1000],\"reservoir_density\" : [0.99], \"leak_rate\" : [0.99], \"Batch_Training\" : [True], \"ReadOut_output_size\" : [25], \"Regularization_L2\" : [0.0]}\n",
    "training_params = {\"num_epochs\" : 1, \"learning_rate\" : 0.01, \"weight_decay\" : 1e-2, \"testdata_ratio\" : 0, \"n_splits\" : 5}\n",
    "\n",
    "#それぞれのモデルパラメータ候補を辞書に格納する\n",
    "model_params = model_params_candinate(model_params)\n",
    "\n",
    "#学習データセットの設定\n",
    "data_dir=\"./normalized_dataset/20251215_161803/\"\n",
    "cross_val_loaders, test_loader = prepare_datasets(dataset_params, training_params, data_dir)\n",
    "\n",
    "# wandb.init(project=\"uskin_test_ESN\", config=config_dictionary)\n",
    "\n",
    "#モデルパラメータの候補ごとに，総当たりしてパラメータを探索する\n",
    "\n",
    "for each_model_params in model_params:\n",
    "    \n",
    "#     model = LSTMModel(each_model_params).to(device)\n",
    "    model = ESN(each_model_params, training_params, dataset_params).to(device)\n",
    "    model_sturcture = model_sturcture_dict(model)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = float(training_params[\"learning_rate\"]), weight_decay = float(training_params[\"weight_decay\"]))\n",
    "    \n",
    "    config_dictionary = {\n",
    "    \"dataset\": data_dir,\n",
    "    \"dataset_params\" : dataset_params,\n",
    "    \"architecture\": model.__class__.__name__,\n",
    "    \"model_params\" : each_model_params,\n",
    "    \"model_sturcture\" : model_sturcture,\n",
    "    \"traing_params\" : training_params,\n",
    "    \"criterion\" : str(criterion),\n",
    "    \"optimizer\" : str(optimizer),\n",
    "    }\n",
    "\n",
    "    wandb.init(project=\"uskin_test_NewESN\", config=config_dictionary)\n",
    "\n",
    "    # 4. k-fold交差検証のループ\n",
    "    for fold, (train_loader, val_loader) in enumerate(cross_val_loaders):\n",
    "        print(f'FOLD {fold}')\n",
    "        print('--------------------------------')\n",
    "        model = ESN(each_model_params, training_params, dataset_params).to(device)\n",
    "        criterion = nn.MSELoss()\n",
    "        train_model(model, criterion, optimizer, train_loader, dataset_params, each_model_params, training_params)\n",
    "        validate_model(model, val_loader, dataset_params, each_model_params, criterion)\n",
    "        # model.__init__(each_model_params, training_params, dataset_params)\n",
    "\n",
    "    print('CrossVaridation Finished')\n",
    "    print('--------------------------------')\n",
    "    #テストデータによる評価\n",
    "    # テストデータローダーの準備\n",
    "\n",
    "#     test_model(model, test_loader)\n",
    "    \n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e7fdfa-9748-469d-9259-ff60e1d869b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
