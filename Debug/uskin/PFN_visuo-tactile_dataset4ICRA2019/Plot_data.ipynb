{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53b839a2-0574-4bcf-949a-41f3c99d87a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: plots/20251215_162635\n",
      "=== Dataset summary ===\n",
      "Found 25 classes:\n",
      "  - 01_table_cover: 10 files\n",
      "      sample shape: (1255, 16, 3) (T, n_taxels, 3)\n",
      "  - 02_fur_scarf: 10 files\n",
      "      sample shape: (1216, 16, 3) (T, n_taxels, 3)\n",
      "  - 03_washing_towel: 10 files\n",
      "      sample shape: (1222, 16, 3) (T, n_taxels, 3)\n",
      "  - 04_carpet1: 10 files\n",
      "      sample shape: (1274, 16, 3) (T, n_taxels, 3)\n",
      "  - 05_bubble_wrap: 10 files\n",
      "      sample shape: (1229, 16, 3) (T, n_taxels, 3)\n",
      "  - 06_fleece_scarf: 10 files\n",
      "      sample shape: (1245, 16, 3) (T, n_taxels, 3)\n",
      "  - 07_knit_hat1: 10 files\n",
      "      sample shape: (1248, 16, 3) (T, n_taxels, 3)\n",
      "  - 08_body_towel1: 10 files\n",
      "      sample shape: (1246, 16, 3) (T, n_taxels, 3)\n",
      "  - 09_body_towel2: 10 files\n",
      "      sample shape: (1247, 16, 3) (T, n_taxels, 3)\n",
      "  - 10_carpet2: 10 files\n",
      "      sample shape: (1278, 16, 3) (T, n_taxels, 3)\n",
      "  - 11_work_gloves: 10 files\n",
      "      sample shape: (1247, 16, 3) (T, n_taxels, 3)\n",
      "  - 12_knit_hat2: 10 files\n",
      "      sample shape: (1243, 16, 3) (T, n_taxels, 3)\n",
      "  - 13_toilet_mat1: 10 files\n",
      "      sample shape: (1270, 16, 3) (T, n_taxels, 3)\n",
      "  - 14_floor_mat: 10 files\n",
      "      sample shape: (1243, 16, 3) (T, n_taxels, 3)\n",
      "  - 15_sponge1: 10 files\n",
      "      sample shape: (1289, 16, 3) (T, n_taxels, 3)\n",
      "  - 16_printed_tatami: 10 files\n",
      "      sample shape: (1224, 16, 3) (T, n_taxels, 3)\n",
      "  - 17_cushion1: 10 files\n",
      "      sample shape: (1276, 16, 3) (T, n_taxels, 3)\n",
      "  - 18_mop: 10 files\n",
      "      sample shape: (1247, 16, 3) (T, n_taxels, 3)\n",
      "  - 19_toilet_mat2: 10 files\n",
      "      sample shape: (1246, 16, 3) (T, n_taxels, 3)\n",
      "  - 20_fleece_sock: 10 files\n",
      "      sample shape: (1253, 16, 3) (T, n_taxels, 3)\n",
      "  - 21_cushion: 10 files\n",
      "      sample shape: (1211, 16, 3) (T, n_taxels, 3)\n",
      "  - 22_carpet3: 10 files\n",
      "      sample shape: (1253, 16, 3) (T, n_taxels, 3)\n",
      "  - 23_fleece_mat: 10 files\n",
      "      sample shape: (1272, 16, 3) (T, n_taxels, 3)\n",
      "  - 24_carpet4: 10 files\n",
      "      sample shape: (1271, 16, 3) (T, n_taxels, 3)\n",
      "  - 25_sponge2: 10 files\n",
      "      sample shape: (1314, 16, 3) (T, n_taxels, 3)\n",
      "=======================\n",
      "\n",
      "Total trials to plot: 75\n",
      "Plotting: class=16_printed_tatami, file=0016_8.npy\n",
      "Saved figure to: plots/20251215_162635/16_printed_tatami_0016_8.png\n",
      "Plotting: class=07_knit_hat1, file=0007_9.npy\n",
      "Saved figure to: plots/20251215_162635/07_knit_hat1_0007_9.png\n",
      "Plotting: class=21_cushion, file=0021_3.npy\n",
      "Saved figure to: plots/20251215_162635/21_cushion_0021_3.png\n",
      "Plotting: class=04_carpet1, file=0004_11.npy\n",
      "Saved figure to: plots/20251215_162635/04_carpet1_0004_11.png\n",
      "Plotting: class=15_sponge1, file=0015_11.npy\n",
      "Saved figure to: plots/20251215_162635/15_sponge1_0015_11.png\n",
      "Plotting: class=01_table_cover, file=0001_11.npy\n",
      "Saved figure to: plots/20251215_162635/01_table_cover_0001_11.png\n",
      "Plotting: class=22_carpet3, file=0022_9.npy\n",
      "Saved figure to: plots/20251215_162635/22_carpet3_0022_9.png\n",
      "Plotting: class=02_fur_scarf, file=0002_11.npy\n",
      "Saved figure to: plots/20251215_162635/02_fur_scarf_0002_11.png\n",
      "Plotting: class=10_carpet2, file=0010_9.npy\n",
      "Saved figure to: plots/20251215_162635/10_carpet2_0010_9.png\n",
      "Plotting: class=10_carpet2, file=0010_10.npy\n",
      "Saved figure to: plots/20251215_162635/10_carpet2_0010_10.png\n",
      "Plotting: class=02_fur_scarf, file=0002_2.npy\n",
      "Saved figure to: plots/20251215_162635/02_fur_scarf_0002_2.png\n",
      "Plotting: class=02_fur_scarf, file=0002_8.npy\n",
      "Saved figure to: plots/20251215_162635/02_fur_scarf_0002_8.png\n",
      "Plotting: class=19_toilet_mat2, file=0019_9.npy\n",
      "Saved figure to: plots/20251215_162635/19_toilet_mat2_0019_9.png\n",
      "Plotting: class=18_mop, file=0018_10.npy\n",
      "Saved figure to: plots/20251215_162635/18_mop_0018_10.png\n",
      "Plotting: class=25_sponge2, file=0025_11.npy\n",
      "Saved figure to: plots/20251215_162635/25_sponge2_0025_11.png\n",
      "Plotting: class=08_body_towel1, file=0008_3.npy\n",
      "Saved figure to: plots/20251215_162635/08_body_towel1_0008_3.png\n",
      "Plotting: class=22_carpet3, file=0022_10.npy\n",
      "Saved figure to: plots/20251215_162635/22_carpet3_0022_10.png\n",
      "Plotting: class=19_toilet_mat2, file=0019_2.npy\n",
      "Saved figure to: plots/20251215_162635/19_toilet_mat2_0019_2.png\n",
      "Plotting: class=22_carpet3, file=0022_4.npy\n",
      "Saved figure to: plots/20251215_162635/22_carpet3_0022_4.png\n",
      "Plotting: class=03_washing_towel, file=0003_9.npy\n",
      "Saved figure to: plots/20251215_162635/03_washing_towel_0003_9.png\n",
      "Plotting: class=04_carpet1, file=0004_7.npy\n",
      "Saved figure to: plots/20251215_162635/04_carpet1_0004_7.png\n",
      "Plotting: class=24_carpet4, file=0024_8.npy\n",
      "Saved figure to: plots/20251215_162635/24_carpet4_0024_8.png\n",
      "Plotting: class=13_toilet_mat1, file=0013_4.npy\n",
      "Saved figure to: plots/20251215_162635/13_toilet_mat1_0013_4.png\n",
      "Plotting: class=15_sponge1, file=0015_7.npy\n",
      "Saved figure to: plots/20251215_162635/15_sponge1_0015_7.png\n",
      "Plotting: class=20_fleece_sock, file=0020_10.npy\n",
      "Saved figure to: plots/20251215_162635/20_fleece_sock_0020_10.png\n",
      "Plotting: class=05_bubble_wrap, file=0005_3.npy\n",
      "Saved figure to: plots/20251215_162635/05_bubble_wrap_0005_3.png\n",
      "Plotting: class=19_toilet_mat2, file=0019_4.npy\n",
      "Saved figure to: plots/20251215_162635/19_toilet_mat2_0019_4.png\n",
      "Plotting: class=10_carpet2, file=0010_2.npy\n",
      "Saved figure to: plots/20251215_162635/10_carpet2_0010_2.png\n",
      "Plotting: class=12_knit_hat2, file=0012_4.npy\n",
      "Saved figure to: plots/20251215_162635/12_knit_hat2_0012_4.png\n",
      "Plotting: class=17_cushion1, file=0017_9.npy\n",
      "Saved figure to: plots/20251215_162635/17_cushion1_0017_9.png\n",
      "Plotting: class=16_printed_tatami, file=0016_11.npy\n",
      "Saved figure to: plots/20251215_162635/16_printed_tatami_0016_11.png\n",
      "Plotting: class=08_body_towel1, file=0008_11.npy\n",
      "Saved figure to: plots/20251215_162635/08_body_towel1_0008_11.png\n",
      "Plotting: class=21_cushion, file=0021_11.npy\n",
      "Saved figure to: plots/20251215_162635/21_cushion_0021_11.png\n",
      "Plotting: class=07_knit_hat1, file=0007_6.npy\n",
      "Saved figure to: plots/20251215_162635/07_knit_hat1_0007_6.png\n",
      "Plotting: class=09_body_towel2, file=0009_10.npy\n",
      "Saved figure to: plots/20251215_162635/09_body_towel2_0009_10.png\n",
      "Plotting: class=12_knit_hat2, file=0012_8.npy\n",
      "Saved figure to: plots/20251215_162635/12_knit_hat2_0012_8.png\n",
      "Plotting: class=14_floor_mat, file=0014_3.npy\n",
      "Saved figure to: plots/20251215_162635/14_floor_mat_0014_3.png\n",
      "Plotting: class=05_bubble_wrap, file=0005_8.npy\n",
      "Saved figure to: plots/20251215_162635/05_bubble_wrap_0005_8.png\n",
      "Plotting: class=03_washing_towel, file=0003_11.npy\n",
      "Saved figure to: plots/20251215_162635/03_washing_towel_0003_11.png\n",
      "Plotting: class=05_bubble_wrap, file=0005_4.npy\n",
      "Saved figure to: plots/20251215_162635/05_bubble_wrap_0005_4.png\n",
      "Plotting: class=16_printed_tatami, file=0016_5.npy\n",
      "Saved figure to: plots/20251215_162635/16_printed_tatami_0016_5.png\n",
      "Plotting: class=20_fleece_sock, file=0020_8.npy\n",
      "Saved figure to: plots/20251215_162635/20_fleece_sock_0020_8.png\n",
      "Plotting: class=14_floor_mat, file=0014_5.npy\n",
      "Saved figure to: plots/20251215_162635/14_floor_mat_0014_5.png\n",
      "Plotting: class=18_mop, file=0018_11.npy\n",
      "Saved figure to: plots/20251215_162635/18_mop_0018_11.png\n",
      "Plotting: class=15_sponge1, file=0015_10.npy\n",
      "Saved figure to: plots/20251215_162635/15_sponge1_0015_10.png\n",
      "Plotting: class=23_fleece_mat, file=0023_9.npy\n",
      "Saved figure to: plots/20251215_162635/23_fleece_mat_0023_9.png\n",
      "Plotting: class=11_work_gloves, file=0011_8.npy\n",
      "Saved figure to: plots/20251215_162635/11_work_gloves_0011_8.png\n",
      "Plotting: class=25_sponge2, file=0025_2.npy\n",
      "Saved figure to: plots/20251215_162635/25_sponge2_0025_2.png\n",
      "Plotting: class=09_body_towel2, file=0009_6.npy\n",
      "Saved figure to: plots/20251215_162635/09_body_towel2_0009_6.png\n",
      "Plotting: class=06_fleece_scarf, file=0006_8.npy\n",
      "Saved figure to: plots/20251215_162635/06_fleece_scarf_0006_8.png\n",
      "Plotting: class=24_carpet4, file=0024_9.npy\n",
      "Saved figure to: plots/20251215_162635/24_carpet4_0024_9.png\n",
      "Plotting: class=03_washing_towel, file=0003_8.npy\n",
      "Saved figure to: plots/20251215_162635/03_washing_towel_0003_8.png\n",
      "Plotting: class=20_fleece_sock, file=0020_2.npy\n",
      "Saved figure to: plots/20251215_162635/20_fleece_sock_0020_2.png\n",
      "Plotting: class=17_cushion1, file=0017_6.npy\n",
      "Saved figure to: plots/20251215_162635/17_cushion1_0017_6.png\n",
      "Plotting: class=18_mop, file=0018_4.npy\n",
      "Saved figure to: plots/20251215_162635/18_mop_0018_4.png\n",
      "Plotting: class=06_fleece_scarf, file=0006_2.npy\n",
      "Saved figure to: plots/20251215_162635/06_fleece_scarf_0006_2.png\n",
      "Plotting: class=07_knit_hat1, file=0007_4.npy\n",
      "Saved figure to: plots/20251215_162635/07_knit_hat1_0007_4.png\n",
      "Plotting: class=06_fleece_scarf, file=0006_6.npy\n",
      "Saved figure to: plots/20251215_162635/06_fleece_scarf_0006_6.png\n",
      "Plotting: class=01_table_cover, file=0001_9.npy\n",
      "Saved figure to: plots/20251215_162635/01_table_cover_0001_9.png\n",
      "Plotting: class=13_toilet_mat1, file=0013_8.npy\n",
      "Saved figure to: plots/20251215_162635/13_toilet_mat1_0013_8.png\n",
      "Plotting: class=04_carpet1, file=0004_4.npy\n",
      "Saved figure to: plots/20251215_162635/04_carpet1_0004_4.png\n",
      "Plotting: class=11_work_gloves, file=0011_10.npy\n",
      "Saved figure to: plots/20251215_162635/11_work_gloves_0011_10.png\n",
      "Plotting: class=23_fleece_mat, file=0023_10.npy\n",
      "Saved figure to: plots/20251215_162635/23_fleece_mat_0023_10.png\n",
      "Plotting: class=12_knit_hat2, file=0012_7.npy\n",
      "Saved figure to: plots/20251215_162635/12_knit_hat2_0012_7.png\n",
      "Plotting: class=21_cushion, file=0021_6.npy\n",
      "Saved figure to: plots/20251215_162635/21_cushion_0021_6.png\n",
      "Plotting: class=08_body_towel1, file=0008_4.npy\n",
      "Saved figure to: plots/20251215_162635/08_body_towel1_0008_4.png\n",
      "Plotting: class=13_toilet_mat1, file=0013_9.npy\n",
      "Saved figure to: plots/20251215_162635/13_toilet_mat1_0013_9.png\n",
      "Plotting: class=24_carpet4, file=0024_4.npy\n",
      "Saved figure to: plots/20251215_162635/24_carpet4_0024_4.png\n",
      "Plotting: class=01_table_cover, file=0001_7.npy\n",
      "Saved figure to: plots/20251215_162635/01_table_cover_0001_7.png\n",
      "Plotting: class=14_floor_mat, file=0014_10.npy\n",
      "Saved figure to: plots/20251215_162635/14_floor_mat_0014_10.png\n",
      "Plotting: class=11_work_gloves, file=0011_3.npy\n",
      "Saved figure to: plots/20251215_162635/11_work_gloves_0011_3.png\n",
      "Plotting: class=17_cushion1, file=0017_4.npy\n",
      "Saved figure to: plots/20251215_162635/17_cushion1_0017_4.png\n",
      "Plotting: class=25_sponge2, file=0025_10.npy\n",
      "Saved figure to: plots/20251215_162635/25_sponge2_0025_10.png\n",
      "Plotting: class=23_fleece_mat, file=0023_2.npy\n",
      "Saved figure to: plots/20251215_162635/23_fleece_mat_0023_2.png\n",
      "Plotting: class=09_body_towel2, file=0009_7.npy\n",
      "Saved figure to: plots/20251215_162635/09_body_towel2_0009_7.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datetime import datetime  # ★ タイムスタンプ用\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ===================== 設定 =====================\n",
    "\n",
    "# ★自分の環境に合わせて書き換えてください\n",
    "DATASET_ROOT = Path(\"./normalized_dataset_forplot/20251215_161803\")  # 触覚データのルート\n",
    "\n",
    "# プロットの範囲（いずれかを選ぶ）\n",
    "PLOT_SCOPE = \"sample_across_classes\"  # \"sample_across_classes\" / \"all_in_one_class\" / \"single_file\"\n",
    "\n",
    "# 1. 全クラスから任意の数のデータをプロット\n",
    "SAMPLES_PER_CLASS = 3      # 各クラスから何ファイルずつ取るか\n",
    "RANDOM_SAMPLE = True       # True: ランダムサンプリング, False: 先頭から順に\n",
    "\n",
    "# 2. あるクラスにおける全てのデータをプロット\n",
    "TARGET_CLASS = \"paper_A4\"  # PLOT_SCOPE=\"all_in_one_class\" のとき使用\n",
    "\n",
    "# 3. 単一ファイルだけプロット\n",
    "SINGLE_CLASS = \"paper_A4\"\n",
    "SINGLE_FILE_INDEX = 0      # そのクラス内で何番目の .npy をプロットするか\n",
    "\n",
    "\n",
    "# ===================== ユーティリティ =====================\n",
    "\n",
    "def list_classes(dataset_root: Path):\n",
    "    \"\"\"クラス名（カテゴリ名）のディレクトリ一覧を返す\"\"\"\n",
    "    classes = [d.name for d in dataset_root.iterdir() if d.is_dir()]\n",
    "    classes.sort()\n",
    "    return classes\n",
    "\n",
    "\n",
    "def list_npy_files(class_dir: Path):\n",
    "    \"\"\"あるクラスディレクトリ内の .npy ファイル一覧を返す\"\"\"\n",
    "    files = [p for p in class_dir.glob(\"*.npy\")]\n",
    "    files.sort()\n",
    "    return files\n",
    "\n",
    "\n",
    "def summarize_dataset(dataset_root: Path):\n",
    "    \"\"\"データセット全体のざっくり概要を標準出力に出す\"\"\"\n",
    "    print(\"=== Dataset summary ===\")\n",
    "    classes = list_classes(dataset_root)\n",
    "    print(f\"Found {len(classes)} classes:\")\n",
    "    for cls in classes:\n",
    "        class_dir = dataset_root / cls\n",
    "        files = list_npy_files(class_dir)\n",
    "        print(f\"  - {cls}: {len(files)} files\")\n",
    "        if files:\n",
    "            sample = np.load(files[0])\n",
    "            print(f\"      sample shape: {sample.shape} (T, n_taxels, 3)\")\n",
    "    print(\"=======================\\n\")\n",
    "\n",
    "\n",
    "# ===================== プロット関数 =====================\n",
    "\n",
    "def plot_all_taxels_grouped_by_axis(data: np.ndarray,\n",
    "                                    class_name: str,\n",
    "                                    file_name: str,\n",
    "                                    output_dir: Path):\n",
    "    \"\"\"\n",
    "    1つの trial (1 .npy ファイル) をプロットする。\n",
    "    data: shape (T, n_taxels, 3)\n",
    "\n",
    "    - X, Y, Z の各軸ごとに1つのサブプロット\n",
    "    - その中に 16タクセル分の線をすべて重ねて描画\n",
    "    - 凡例はグラフにかぶらないように右側に配置\n",
    "    - 画像を output_dir に保存する\n",
    "    \"\"\"\n",
    "    assert data.ndim == 3, f\"Expected 3D array, got {data.shape}\"\n",
    "    T, n_taxels, n_axes = data.shape\n",
    "    assert n_axes == 3, f\"Last dim must be 3 (x, y, z), got {n_axes}\"\n",
    "\n",
    "    time = np.arange(T)\n",
    "    axis_names = [\"x\", \"y\", \"z\"]\n",
    "\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(10, 8), sharex=True)\n",
    "    if not isinstance(axes, np.ndarray):\n",
    "        axes = np.array([axes])\n",
    "\n",
    "    for axis in range(3):\n",
    "        ax = axes[axis]\n",
    "        for taxel in range(n_taxels):\n",
    "            # 凡例をすっきりさせるため、ラベルは最初のサブプロットだけ\n",
    "            label = f\"taxel {taxel}\" if axis == 0 else \"_nolegend_\"\n",
    "            ax.plot(time,\n",
    "                    data[:, taxel, axis],\n",
    "                    alpha=0.6,\n",
    "                    linewidth=0.8,\n",
    "                    label=label)\n",
    "        ax.set_ylabel(f\"{axis_names[axis]} (arb.)\")\n",
    "        ax.grid(True)\n",
    "\n",
    "    axes[-1].set_xlabel(\"Time step\")\n",
    "\n",
    "    # 凡例は図の右側に配置して、グラフに重ならないようにする\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    fig.suptitle(f\"{class_name} | {file_name}\")\n",
    "    # 左側 85% をプロット用に確保し、右 15% を凡例用に空ける\n",
    "    fig.tight_layout(rect=(0.0, 0.0, 0.85, 0.95))\n",
    "\n",
    "    if handles:\n",
    "        fig.legend(handles,\n",
    "                   labels,\n",
    "                   loc=\"center left\",\n",
    "                   bbox_to_anchor=(0.88, 0.5),\n",
    "                   borderaxespad=0.)\n",
    "\n",
    "    # ==== ここから保存処理 ====\n",
    "    stem = Path(file_name).stem\n",
    "    out_name = f\"{class_name}_{stem}.png\"\n",
    "    out_path = output_dir / out_name\n",
    "    fig.savefig(out_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"Saved figure to: {out_path}\")\n",
    "\n",
    "\n",
    "def plot_trials(trial_paths, output_dir: Path):\n",
    "    \"\"\"\n",
    "    複数の npy ファイルを順番に読み込み、各 trial ごとに\n",
    "    「XYZ軸ごとに16タクセルをまとめて」プロットし、保存する。\n",
    "    trial_paths: list[Path]\n",
    "    \"\"\"\n",
    "    for path in trial_paths:\n",
    "        class_name = path.parent.name\n",
    "        file_name = path.name\n",
    "        print(f\"Plotting: class={class_name}, file={file_name}\")\n",
    "        data = np.load(path)\n",
    "        plot_all_taxels_grouped_by_axis(data, class_name, file_name, output_dir)\n",
    "\n",
    "\n",
    "# ===================== メイン処理 =====================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ★ タイムスタンプ付きの出力ディレクトリを作成\n",
    "    timestamp_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    OUTPUT_ROOT = Path(\"./plots\")\n",
    "    RUN_OUTPUT_DIR = OUTPUT_ROOT / timestamp_str\n",
    "    RUN_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Output directory: {RUN_OUTPUT_DIR}\")\n",
    "\n",
    "    summarize_dataset(DATASET_ROOT)\n",
    "    classes = list_classes(DATASET_ROOT)\n",
    "    if not classes:\n",
    "        raise RuntimeError(\"No class directories found under DATASET_ROOT\")\n",
    "\n",
    "    trial_paths = []\n",
    "\n",
    "    if PLOT_SCOPE == \"sample_across_classes\":\n",
    "        # 全クラスから任意個のファイルをピックアップ\n",
    "        for cls in classes:\n",
    "            class_dir = DATASET_ROOT / cls\n",
    "            files = list_npy_files(class_dir)\n",
    "            if not files:\n",
    "                continue\n",
    "\n",
    "            if RANDOM_SAMPLE:\n",
    "                n = min(SAMPLES_PER_CLASS, len(files))\n",
    "                sampled = random.sample(files, n)\n",
    "            else:\n",
    "                sampled = files[:SAMPLES_PER_CLASS]\n",
    "\n",
    "            trial_paths.extend(sampled)\n",
    "\n",
    "        # （必要ならシャッフル）\n",
    "        random.shuffle(trial_paths)\n",
    "\n",
    "    elif PLOT_SCOPE == \"all_in_one_class\":\n",
    "        # あるクラスに含まれる全ファイルをプロット\n",
    "        if TARGET_CLASS not in classes:\n",
    "            raise ValueError(f\"TARGET_CLASS '{TARGET_CLASS}' not found. Available: {classes}\")\n",
    "        class_dir = DATASET_ROOT / TARGET_CLASS\n",
    "        trial_paths = list_npy_files(class_dir)\n",
    "        if not trial_paths:\n",
    "            raise RuntimeError(f\"No .npy files in class directory: {class_dir}\")\n",
    "\n",
    "    elif PLOT_SCOPE == \"single_file\":\n",
    "        # 単一ファイルのみプロット\n",
    "        if SINGLE_CLASS not in classes:\n",
    "            raise ValueError(f\"SINGLE_CLASS '{SINGLE_CLASS}' not found. Available: {classes}\")\n",
    "        class_dir = DATASET_ROOT / SINGLE_CLASS\n",
    "        files = list_npy_files(class_dir)\n",
    "        if not files:\n",
    "            raise RuntimeError(f\"No .npy files in class directory: {class_dir}\")\n",
    "        if not (0 <= SINGLE_FILE_INDEX < len(files)):\n",
    "            raise ValueError(f\"SINGLE_FILE_INDEX {SINGLE_FILE_INDEX} out of range (0..{len(files)-1})\")\n",
    "\n",
    "        trial_paths = [files[SINGLE_FILE_INDEX]]\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown PLOT_SCOPE: {PLOT_SCOPE}\")\n",
    "\n",
    "    # 実際にプロット＆保存\n",
    "    print(f\"Total trials to plot: {len(trial_paths)}\")\n",
    "    plot_trials(trial_paths, RUN_OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7177ca8f-ee58-4642-b41b-a7edb4934f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "####データセット全体にて標準化し，プロット\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# ルートディレクトリ（クラス名ディレクトリが並んでいる場所）\n",
    "DATASET_ROOT = \"./dataset_root\"\n",
    "\n",
    "# Dataset を作成\n",
    "dataset = TactileSequenceDataset(\n",
    "    root_dir=DATASET_ROOT,\n",
    "    seq_start=400,\n",
    "    seq_end=1200,\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "\n",
    "# 7:2:1 くらいで train/val/test に分割する例\n",
    "total_len = len(dataset)\n",
    "train_len = int(total_len * 0.7)\n",
    "val_len   = int(total_len * 0.2)\n",
    "test_len  = total_len - train_len - val_len\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset, [train_len, val_len, test_len],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "print(\"train_batches:\", len(train_loader))\n",
    "print(\"val_batches  :\", len(val_loader))\n",
    "print(\"test_batches :\", len(test_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "877bf775-470c-48ff-ba93-ae4da661e790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: plots/20251215_155716\n",
      "=== Dataset summary ===\n",
      "Found 25 classes:\n",
      "  - 01_table_cover: 10 files\n",
      "      sample shape: (1255, 16, 3) (T, n_taxels, 3)\n",
      "  - 02_fur_scarf: 10 files\n",
      "      sample shape: (1216, 16, 3) (T, n_taxels, 3)\n",
      "  - 03_washing_towel: 10 files\n",
      "      sample shape: (1222, 16, 3) (T, n_taxels, 3)\n",
      "  - 04_carpet1: 10 files\n",
      "      sample shape: (1274, 16, 3) (T, n_taxels, 3)\n",
      "  - 05_bubble_wrap: 10 files\n",
      "      sample shape: (1229, 16, 3) (T, n_taxels, 3)\n",
      "  - 06_fleece_scarf: 10 files\n",
      "      sample shape: (1245, 16, 3) (T, n_taxels, 3)\n",
      "  - 07_knit_hat1: 10 files\n",
      "      sample shape: (1248, 16, 3) (T, n_taxels, 3)\n",
      "  - 08_body_towel1: 10 files\n",
      "      sample shape: (1246, 16, 3) (T, n_taxels, 3)\n",
      "  - 09_body_towel2: 10 files\n",
      "      sample shape: (1247, 16, 3) (T, n_taxels, 3)\n",
      "  - 10_carpet2: 10 files\n",
      "      sample shape: (1278, 16, 3) (T, n_taxels, 3)\n",
      "  - 11_work_gloves: 10 files\n",
      "      sample shape: (1247, 16, 3) (T, n_taxels, 3)\n",
      "  - 12_knit_hat2: 10 files\n",
      "      sample shape: (1243, 16, 3) (T, n_taxels, 3)\n",
      "  - 13_toilet_mat1: 10 files\n",
      "      sample shape: (1270, 16, 3) (T, n_taxels, 3)\n",
      "  - 14_floor_mat: 10 files\n",
      "      sample shape: (1243, 16, 3) (T, n_taxels, 3)\n",
      "  - 15_sponge1: 10 files\n",
      "      sample shape: (1289, 16, 3) (T, n_taxels, 3)\n",
      "  - 16_printed_tatami: 10 files\n",
      "      sample shape: (1224, 16, 3) (T, n_taxels, 3)\n",
      "  - 17_cushion1: 10 files\n",
      "      sample shape: (1276, 16, 3) (T, n_taxels, 3)\n",
      "  - 18_mop: 10 files\n",
      "      sample shape: (1247, 16, 3) (T, n_taxels, 3)\n",
      "  - 19_toilet_mat2: 10 files\n",
      "      sample shape: (1246, 16, 3) (T, n_taxels, 3)\n",
      "  - 20_fleece_sock: 10 files\n",
      "      sample shape: (1253, 16, 3) (T, n_taxels, 3)\n",
      "  - 21_cushion: 10 files\n",
      "      sample shape: (1211, 16, 3) (T, n_taxels, 3)\n",
      "  - 22_carpet3: 10 files\n",
      "      sample shape: (1253, 16, 3) (T, n_taxels, 3)\n",
      "  - 23_fleece_mat: 10 files\n",
      "      sample shape: (1272, 16, 3) (T, n_taxels, 3)\n",
      "  - 24_carpet4: 10 files\n",
      "      sample shape: (1271, 16, 3) (T, n_taxels, 3)\n",
      "  - 25_sponge2: 10 files\n",
      "      sample shape: (1314, 16, 3) (T, n_taxels, 3)\n",
      "=======================\n",
      "\n",
      "\n",
      "[compute] global mean/std with offset t=0..100 (per feature: taxel×axis)\n",
      "[global-stats] processed 200/250 files...\n",
      "[done] global stats computed.\n",
      "  mean shape: (16, 3) std shape: (16, 3)\n",
      "  std min/max: 280.9351501464844 4830.4208984375\n",
      "Saved normalization stats to: plots/20251215_155716\n",
      "Total trials to plot: 75\n",
      "Plotting: class=10_carpet2, file=0010_7.npy\n",
      "Saved figure to: plots/20251215_155716/10_carpet2_0010_7.png\n",
      "Plotting: class=08_body_towel1, file=0008_4.npy\n",
      "Saved figure to: plots/20251215_155716/08_body_towel1_0008_4.png\n",
      "Plotting: class=15_sponge1, file=0015_5.npy\n",
      "Saved figure to: plots/20251215_155716/15_sponge1_0015_5.png\n",
      "Plotting: class=03_washing_towel, file=0003_6.npy\n",
      "Saved figure to: plots/20251215_155716/03_washing_towel_0003_6.png\n",
      "Plotting: class=01_table_cover, file=0001_6.npy\n",
      "Saved figure to: plots/20251215_155716/01_table_cover_0001_6.png\n",
      "Plotting: class=24_carpet4, file=0024_7.npy\n",
      "Saved figure to: plots/20251215_155716/24_carpet4_0024_7.png\n",
      "Plotting: class=12_knit_hat2, file=0012_11.npy\n",
      "Saved figure to: plots/20251215_155716/12_knit_hat2_0012_11.png\n",
      "Plotting: class=22_carpet3, file=0022_10.npy\n",
      "Saved figure to: plots/20251215_155716/22_carpet3_0022_10.png\n",
      "Plotting: class=21_cushion, file=0021_10.npy\n",
      "Saved figure to: plots/20251215_155716/21_cushion_0021_10.png\n",
      "Plotting: class=20_fleece_sock, file=0020_10.npy\n",
      "Saved figure to: plots/20251215_155716/20_fleece_sock_0020_10.png\n",
      "Plotting: class=14_floor_mat, file=0014_10.npy\n",
      "Saved figure to: plots/20251215_155716/14_floor_mat_0014_10.png\n",
      "Plotting: class=24_carpet4, file=0024_3.npy\n",
      "Saved figure to: plots/20251215_155716/24_carpet4_0024_3.png\n",
      "Plotting: class=15_sponge1, file=0015_8.npy\n",
      "Saved figure to: plots/20251215_155716/15_sponge1_0015_8.png\n",
      "Plotting: class=14_floor_mat, file=0014_3.npy\n",
      "Saved figure to: plots/20251215_155716/14_floor_mat_0014_3.png\n",
      "Plotting: class=23_fleece_mat, file=0023_3.npy\n",
      "Saved figure to: plots/20251215_155716/23_fleece_mat_0023_3.png\n",
      "Plotting: class=18_mop, file=0018_6.npy\n",
      "Saved figure to: plots/20251215_155716/18_mop_0018_6.png\n",
      "Plotting: class=02_fur_scarf, file=0002_2.npy\n",
      "Saved figure to: plots/20251215_155716/02_fur_scarf_0002_2.png\n",
      "Plotting: class=21_cushion, file=0021_8.npy\n",
      "Saved figure to: plots/20251215_155716/21_cushion_0021_8.png\n",
      "Plotting: class=10_carpet2, file=0010_8.npy\n",
      "Saved figure to: plots/20251215_155716/10_carpet2_0010_8.png\n",
      "Plotting: class=07_knit_hat1, file=0007_11.npy\n",
      "Saved figure to: plots/20251215_155716/07_knit_hat1_0007_11.png\n",
      "Plotting: class=20_fleece_sock, file=0020_2.npy\n",
      "Saved figure to: plots/20251215_155716/20_fleece_sock_0020_2.png\n",
      "Plotting: class=14_floor_mat, file=0014_9.npy\n",
      "Saved figure to: plots/20251215_155716/14_floor_mat_0014_9.png\n",
      "Plotting: class=12_knit_hat2, file=0012_8.npy\n",
      "Saved figure to: plots/20251215_155716/12_knit_hat2_0012_8.png\n",
      "Plotting: class=19_toilet_mat2, file=0019_5.npy\n",
      "Saved figure to: plots/20251215_155716/19_toilet_mat2_0019_5.png\n",
      "Plotting: class=17_cushion1, file=0017_5.npy\n",
      "Saved figure to: plots/20251215_155716/17_cushion1_0017_5.png\n",
      "Plotting: class=24_carpet4, file=0024_4.npy\n",
      "Saved figure to: plots/20251215_155716/24_carpet4_0024_4.png\n",
      "Plotting: class=19_toilet_mat2, file=0019_10.npy\n",
      "Saved figure to: plots/20251215_155716/19_toilet_mat2_0019_10.png\n",
      "Plotting: class=06_fleece_scarf, file=0006_7.npy\n",
      "Saved figure to: plots/20251215_155716/06_fleece_scarf_0006_7.png\n",
      "Plotting: class=16_printed_tatami, file=0016_2.npy\n",
      "Saved figure to: plots/20251215_155716/16_printed_tatami_0016_2.png\n",
      "Plotting: class=25_sponge2, file=0025_2.npy\n",
      "Saved figure to: plots/20251215_155716/25_sponge2_0025_2.png\n",
      "Plotting: class=09_body_towel2, file=0009_8.npy\n",
      "Saved figure to: plots/20251215_155716/09_body_towel2_0009_8.png\n",
      "Plotting: class=08_body_towel1, file=0008_10.npy\n",
      "Saved figure to: plots/20251215_155716/08_body_towel1_0008_10.png\n",
      "Plotting: class=11_work_gloves, file=0011_3.npy\n",
      "Saved figure to: plots/20251215_155716/11_work_gloves_0011_3.png\n",
      "Plotting: class=23_fleece_mat, file=0023_11.npy\n",
      "Saved figure to: plots/20251215_155716/23_fleece_mat_0023_11.png\n",
      "Plotting: class=18_mop, file=0018_8.npy\n",
      "Saved figure to: plots/20251215_155716/18_mop_0018_8.png\n",
      "Plotting: class=02_fur_scarf, file=0002_10.npy\n",
      "Saved figure to: plots/20251215_155716/02_fur_scarf_0002_10.png\n",
      "Plotting: class=05_bubble_wrap, file=0005_9.npy\n",
      "Saved figure to: plots/20251215_155716/05_bubble_wrap_0005_9.png\n",
      "Plotting: class=18_mop, file=0018_4.npy\n",
      "Saved figure to: plots/20251215_155716/18_mop_0018_4.png\n",
      "Plotting: class=15_sponge1, file=0015_4.npy\n",
      "Saved figure to: plots/20251215_155716/15_sponge1_0015_4.png\n",
      "Plotting: class=25_sponge2, file=0025_11.npy\n",
      "Saved figure to: plots/20251215_155716/25_sponge2_0025_11.png\n",
      "Plotting: class=09_body_towel2, file=0009_5.npy\n",
      "Saved figure to: plots/20251215_155716/09_body_towel2_0009_5.png\n",
      "Plotting: class=13_toilet_mat1, file=0013_4.npy\n",
      "Saved figure to: plots/20251215_155716/13_toilet_mat1_0013_4.png\n",
      "Plotting: class=13_toilet_mat1, file=0013_10.npy\n",
      "Saved figure to: plots/20251215_155716/13_toilet_mat1_0013_10.png\n",
      "Plotting: class=06_fleece_scarf, file=0006_4.npy\n",
      "Saved figure to: plots/20251215_155716/06_fleece_scarf_0006_4.png\n",
      "Plotting: class=04_carpet1, file=0004_2.npy\n",
      "Saved figure to: plots/20251215_155716/04_carpet1_0004_2.png\n",
      "Plotting: class=16_printed_tatami, file=0016_9.npy\n",
      "Saved figure to: plots/20251215_155716/16_printed_tatami_0016_9.png\n",
      "Plotting: class=01_table_cover, file=0001_2.npy\n",
      "Saved figure to: plots/20251215_155716/01_table_cover_0001_2.png\n",
      "Plotting: class=09_body_towel2, file=0009_10.npy\n",
      "Saved figure to: plots/20251215_155716/09_body_towel2_0009_10.png\n",
      "Plotting: class=11_work_gloves, file=0011_6.npy\n",
      "Saved figure to: plots/20251215_155716/11_work_gloves_0011_6.png\n",
      "Plotting: class=02_fur_scarf, file=0002_8.npy\n",
      "Saved figure to: plots/20251215_155716/02_fur_scarf_0002_8.png\n",
      "Plotting: class=13_toilet_mat1, file=0013_8.npy\n",
      "Saved figure to: plots/20251215_155716/13_toilet_mat1_0013_8.png\n",
      "Plotting: class=25_sponge2, file=0025_6.npy\n",
      "Saved figure to: plots/20251215_155716/25_sponge2_0025_6.png\n",
      "Plotting: class=03_washing_towel, file=0003_4.npy\n",
      "Saved figure to: plots/20251215_155716/03_washing_towel_0003_4.png\n",
      "Plotting: class=04_carpet1, file=0004_11.npy\n",
      "Saved figure to: plots/20251215_155716/04_carpet1_0004_11.png\n",
      "Plotting: class=05_bubble_wrap, file=0005_7.npy\n",
      "Saved figure to: plots/20251215_155716/05_bubble_wrap_0005_7.png\n",
      "Plotting: class=22_carpet3, file=0022_3.npy\n",
      "Saved figure to: plots/20251215_155716/22_carpet3_0022_3.png\n",
      "Plotting: class=06_fleece_scarf, file=0006_2.npy\n",
      "Saved figure to: plots/20251215_155716/06_fleece_scarf_0006_2.png\n",
      "Plotting: class=03_washing_towel, file=0003_2.npy\n",
      "Saved figure to: plots/20251215_155716/03_washing_towel_0003_2.png\n",
      "Plotting: class=20_fleece_sock, file=0020_5.npy\n",
      "Saved figure to: plots/20251215_155716/20_fleece_sock_0020_5.png\n",
      "Plotting: class=19_toilet_mat2, file=0019_2.npy\n",
      "Saved figure to: plots/20251215_155716/19_toilet_mat2_0019_2.png\n",
      "Plotting: class=22_carpet3, file=0022_9.npy\n",
      "Saved figure to: plots/20251215_155716/22_carpet3_0022_9.png\n",
      "Plotting: class=10_carpet2, file=0010_3.npy\n",
      "Saved figure to: plots/20251215_155716/10_carpet2_0010_3.png\n",
      "Plotting: class=05_bubble_wrap, file=0005_6.npy\n",
      "Saved figure to: plots/20251215_155716/05_bubble_wrap_0005_6.png\n",
      "Plotting: class=23_fleece_mat, file=0023_10.npy\n",
      "Saved figure to: plots/20251215_155716/23_fleece_mat_0023_10.png\n",
      "Plotting: class=16_printed_tatami, file=0016_8.npy\n",
      "Saved figure to: plots/20251215_155716/16_printed_tatami_0016_8.png\n",
      "Plotting: class=08_body_towel1, file=0008_6.npy\n",
      "Saved figure to: plots/20251215_155716/08_body_towel1_0008_6.png\n",
      "Plotting: class=21_cushion, file=0021_3.npy\n",
      "Saved figure to: plots/20251215_155716/21_cushion_0021_3.png\n",
      "Plotting: class=07_knit_hat1, file=0007_6.npy\n",
      "Saved figure to: plots/20251215_155716/07_knit_hat1_0007_6.png\n",
      "Plotting: class=01_table_cover, file=0001_8.npy\n",
      "Saved figure to: plots/20251215_155716/01_table_cover_0001_8.png\n",
      "Plotting: class=04_carpet1, file=0004_9.npy\n",
      "Saved figure to: plots/20251215_155716/04_carpet1_0004_9.png\n",
      "Plotting: class=12_knit_hat2, file=0012_5.npy\n",
      "Saved figure to: plots/20251215_155716/12_knit_hat2_0012_5.png\n",
      "Plotting: class=17_cushion1, file=0017_11.npy\n",
      "Saved figure to: plots/20251215_155716/17_cushion1_0017_11.png\n",
      "Plotting: class=11_work_gloves, file=0011_11.npy\n",
      "Saved figure to: plots/20251215_155716/11_work_gloves_0011_11.png\n",
      "Plotting: class=17_cushion1, file=0017_7.npy\n",
      "Saved figure to: plots/20251215_155716/17_cushion1_0017_7.png\n",
      "Plotting: class=07_knit_hat1, file=0007_7.npy\n",
      "Saved figure to: plots/20251215_155716/07_knit_hat1_0007_7.png\n"
     ]
    }
   ],
   "source": [
    "####データセット全体にて標準化し，プロット\n",
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ===================== 設定 =====================\n",
    "\n",
    "DATASET_ROOT = Path(\"./All_materials\")  # 触覚データのルート\n",
    "\n",
    "PLOT_SCOPE = \"sample_across_classes\"  # \"sample_across_classes\" / \"all_in_one_class\" / \"single_file\"\n",
    "\n",
    "SAMPLES_PER_CLASS = 3\n",
    "RANDOM_SAMPLE = True\n",
    "\n",
    "TARGET_CLASS = \"paper_A4\"  # PLOT_SCOPE=\"all_in_one_class\" のとき使用\n",
    "\n",
    "SINGLE_CLASS = \"paper_A4\"\n",
    "SINGLE_FILE_INDEX = 0\n",
    "\n",
    "# ---- 正規化設定（追加） ----\n",
    "APPLY_GLOBAL_ZSCORE = True\n",
    "\n",
    "OFFSET_T0 = 0\n",
    "OFFSET_T1 = 100  # 0..100 を基準（両端含む）\n",
    "EPS = 1e-8\n",
    "\n",
    "\n",
    "# ===================== ユーティリティ =====================\n",
    "\n",
    "def list_classes(dataset_root: Path):\n",
    "    classes = [d.name for d in dataset_root.iterdir() if d.is_dir()]\n",
    "    classes.sort()\n",
    "    return classes\n",
    "\n",
    "\n",
    "def list_npy_files(class_dir: Path):\n",
    "    files = [p for p in class_dir.glob(\"*.npy\")]\n",
    "    files.sort()\n",
    "    return files\n",
    "\n",
    "\n",
    "def iter_all_npy_files(dataset_root: Path):\n",
    "    \"\"\"DATASET_ROOT配下の全 .npy を列挙（クラス横断）\"\"\"\n",
    "    for cls in list_classes(dataset_root):\n",
    "        class_dir = dataset_root / cls\n",
    "        for p in list_npy_files(class_dir):\n",
    "            yield p\n",
    "\n",
    "\n",
    "def summarize_dataset(dataset_root: Path):\n",
    "    print(\"=== Dataset summary ===\")\n",
    "    classes = list_classes(dataset_root)\n",
    "    print(f\"Found {len(classes)} classes:\")\n",
    "    for cls in classes:\n",
    "        class_dir = dataset_root / cls\n",
    "        files = list_npy_files(class_dir)\n",
    "        print(f\"  - {cls}: {len(files)} files\")\n",
    "        if files:\n",
    "            sample = np.load(files[0])\n",
    "            print(f\"      sample shape: {sample.shape} (T, n_taxels, 3)\")\n",
    "    print(\"=======================\\n\")\n",
    "\n",
    "\n",
    "# ===================== 正規化（追加） =====================\n",
    "\n",
    "def compute_global_stats_with_offset(dataset_root: Path,\n",
    "                                     offset_t0: int = 0,\n",
    "                                     offset_t1: int = 100,\n",
    "                                     eps: float = 1e-8):\n",
    "    \"\"\"\n",
    "    1) 各trialでオフセット除去（t=offset_t0..offset_t1 の平均を引く）\n",
    "    2) オフセット除去後の値を用いて、全 .npy を対象に global mean/std を\n",
    "       各feature（taxel×軸）次元ごとに推定する。\n",
    "\n",
    "    Returns:\n",
    "      mean: shape (n_taxels, 3)\n",
    "      std : shape (n_taxels, 3)\n",
    "    \"\"\"\n",
    "    sum_ = None\n",
    "    sumsq_ = None\n",
    "    total_count = 0\n",
    "\n",
    "    n_taxels_ref = None\n",
    "\n",
    "    paths = list(iter_all_npy_files(dataset_root))\n",
    "    if not paths:\n",
    "        raise RuntimeError(f\"No .npy files found under: {dataset_root}\")\n",
    "\n",
    "    for i, path in enumerate(paths):\n",
    "        data = np.load(path)  # (T, n_taxels, 3)\n",
    "        if data.ndim != 3 or data.shape[-1] != 3:\n",
    "            raise ValueError(f\"Invalid shape {data.shape} in {path} (expected (T, n_taxels, 3))\")\n",
    "\n",
    "        T, n_taxels, _ = data.shape\n",
    "        if n_taxels_ref is None:\n",
    "            n_taxels_ref = n_taxels\n",
    "            sum_ = np.zeros((n_taxels_ref, 3), dtype=np.float64)\n",
    "            sumsq_ = np.zeros((n_taxels_ref, 3), dtype=np.float64)\n",
    "        else:\n",
    "            if n_taxels != n_taxels_ref:\n",
    "                raise ValueError(f\"n_taxels mismatch: {n_taxels} != {n_taxels_ref} in {path}\")\n",
    "\n",
    "        if T <= offset_t1:\n",
    "            raise ValueError(f\"T={T} is too short for offset range 0..{offset_t1} in {path}\")\n",
    "\n",
    "        # trialごとのオフセット（taxel×軸ごと）\n",
    "        baseline = data[offset_t0:offset_t1 + 1].mean(axis=0, keepdims=True)  # (1, n_taxels, 3)\n",
    "        data0 = (data - baseline).astype(np.float64)  # (T, n_taxels, 3)\n",
    "\n",
    "        sum_ += data0.sum(axis=0)               # (n_taxels, 3)\n",
    "        sumsq_ += (data0 * data0).sum(axis=0)   # (n_taxels, 3)\n",
    "        total_count += T\n",
    "\n",
    "        if (i + 1) % 200 == 0:\n",
    "            print(f\"[global-stats] processed {i+1}/{len(paths)} files...\")\n",
    "\n",
    "    mean = sum_ / total_count\n",
    "    var = (sumsq_ / total_count) - (mean * mean)\n",
    "    var = np.maximum(var, 0.0)\n",
    "    std = np.sqrt(var + eps)\n",
    "\n",
    "    return mean.astype(np.float32), std.astype(np.float32)\n",
    "\n",
    "\n",
    "def apply_offset_and_global_zscore(data: np.ndarray,\n",
    "                                  mean: np.ndarray,\n",
    "                                  std: np.ndarray,\n",
    "                                  offset_t0: int = 0,\n",
    "                                  offset_t1: int = 100):\n",
    "    \"\"\"\n",
    "    data: (T, n_taxels, 3)\n",
    "    mean/std: (n_taxels, 3)  ※オフセット除去後のglobal統計\n",
    "    \"\"\"\n",
    "    T, n_taxels, _ = data.shape\n",
    "    if T <= offset_t1:\n",
    "        raise ValueError(f\"T={T} is too short for offset range 0..{offset_t1}\")\n",
    "\n",
    "    baseline = data[offset_t0:offset_t1 + 1].mean(axis=0, keepdims=True)  # (1, n_taxels, 3)\n",
    "    data0 = data - baseline\n",
    "    data_norm = (data0 - mean[None, :, :]) / std[None, :, :]\n",
    "    return data_norm\n",
    "\n",
    "\n",
    "# ===================== プロット関数（元のまま） =====================\n",
    "\n",
    "def plot_all_taxels_grouped_by_axis(data: np.ndarray,\n",
    "                                    class_name: str,\n",
    "                                    file_name: str,\n",
    "                                    output_dir: Path):\n",
    "    assert data.ndim == 3, f\"Expected 3D array, got {data.shape}\"\n",
    "    T, n_taxels, n_axes = data.shape\n",
    "    assert n_axes == 3, f\"Last dim must be 3 (x, y, z), got {n_axes}\"\n",
    "\n",
    "    time = np.arange(T)\n",
    "    axis_names = [\"x\", \"y\", \"z\"]\n",
    "\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(10, 8), sharex=True)\n",
    "    if not isinstance(axes, np.ndarray):\n",
    "        axes = np.array([axes])\n",
    "\n",
    "    for axis in range(3):\n",
    "        ax = axes[axis]\n",
    "        for taxel in range(n_taxels):\n",
    "            label = f\"taxel {taxel}\" if axis == 0 else \"_nolegend_\"\n",
    "            ax.plot(time,\n",
    "                    data[:, taxel, axis],\n",
    "                    alpha=0.6,\n",
    "                    linewidth=0.8,\n",
    "                    label=label)\n",
    "        ax.set_ylabel(f\"{axis_names[axis]} (arb.)\")\n",
    "        ax.grid(True)\n",
    "\n",
    "    axes[-1].set_xlabel(\"Time step\")\n",
    "\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    fig.suptitle(f\"{class_name} | {file_name}\")\n",
    "    fig.tight_layout(rect=(0.0, 0.0, 0.85, 0.95))\n",
    "\n",
    "    if handles:\n",
    "        fig.legend(handles,\n",
    "                   labels,\n",
    "                   loc=\"center left\",\n",
    "                   bbox_to_anchor=(0.88, 0.5),\n",
    "                   borderaxespad=0.)\n",
    "\n",
    "    stem = Path(file_name).stem\n",
    "    out_name = f\"{class_name}_{stem}.png\"\n",
    "    out_path = output_dir / out_name\n",
    "    fig.savefig(out_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"Saved figure to: {out_path}\")\n",
    "\n",
    "\n",
    "def plot_trials(trial_paths, output_dir: Path, global_mean=None, global_std=None):\n",
    "    for path in trial_paths:\n",
    "        class_name = path.parent.name\n",
    "        file_name = path.name\n",
    "        print(f\"Plotting: class={class_name}, file={file_name}\")\n",
    "\n",
    "        data = np.load(path)  # (T, n_taxels, 3)\n",
    "\n",
    "        if APPLY_GLOBAL_ZSCORE:\n",
    "            if global_mean is None or global_std is None:\n",
    "                raise RuntimeError(\"global_mean/std are required when APPLY_GLOBAL_ZSCORE=True\")\n",
    "            data = apply_offset_and_global_zscore(\n",
    "                data,\n",
    "                mean=global_mean,\n",
    "                std=global_std,\n",
    "                offset_t0=OFFSET_T0,\n",
    "                offset_t1=OFFSET_T1\n",
    "            )\n",
    "\n",
    "        plot_all_taxels_grouped_by_axis(data, class_name, file_name, output_dir)\n",
    "\n",
    "\n",
    "# ===================== メイン処理 =====================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    timestamp_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    OUTPUT_ROOT = Path(\"./plots\")\n",
    "    RUN_OUTPUT_DIR = OUTPUT_ROOT / timestamp_str\n",
    "    RUN_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Output directory: {RUN_OUTPUT_DIR}\")\n",
    "\n",
    "    summarize_dataset(DATASET_ROOT)\n",
    "    classes = list_classes(DATASET_ROOT)\n",
    "    if not classes:\n",
    "        raise RuntimeError(\"No class directories found under DATASET_ROOT\")\n",
    "\n",
    "    # ---- 追加：global統計量の推定（データセット全体） ----\n",
    "    global_mean, global_std = None, None\n",
    "    if APPLY_GLOBAL_ZSCORE:\n",
    "        print(f\"\\n[compute] global mean/std with offset t={OFFSET_T0}..{OFFSET_T1} (per feature: taxel×axis)\")\n",
    "        global_mean, global_std = compute_global_stats_with_offset(\n",
    "            DATASET_ROOT, offset_t0=OFFSET_T0, offset_t1=OFFSET_T1, eps=EPS\n",
    "        )\n",
    "        print(\"[done] global stats computed.\")\n",
    "        print(\"  mean shape:\", global_mean.shape, \"std shape:\", global_std.shape)\n",
    "        print(\"  std min/max:\", float(global_std.min()), float(global_std.max()))\n",
    "\n",
    "        # 再現性のため保存（任意）\n",
    "        np.save(RUN_OUTPUT_DIR / \"global_mean_taxel_axis.npy\", global_mean)\n",
    "        np.save(RUN_OUTPUT_DIR / \"global_std_taxel_axis.npy\", global_std)\n",
    "        with open(RUN_OUTPUT_DIR / \"normalization_info.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"APPLY_GLOBAL_ZSCORE={APPLY_GLOBAL_ZSCORE}\\n\")\n",
    "            f.write(f\"OFFSET_T0={OFFSET_T0}\\n\")\n",
    "            f.write(f\"OFFSET_T1={OFFSET_T1}\\n\")\n",
    "            f.write(f\"EPS={EPS}\\n\")\n",
    "        print(f\"Saved normalization stats to: {RUN_OUTPUT_DIR}\")\n",
    "\n",
    "    trial_paths = []\n",
    "\n",
    "    if PLOT_SCOPE == \"sample_across_classes\":\n",
    "        for cls in classes:\n",
    "            class_dir = DATASET_ROOT / cls\n",
    "            files = list_npy_files(class_dir)\n",
    "            if not files:\n",
    "                continue\n",
    "\n",
    "            if RANDOM_SAMPLE:\n",
    "                n = min(SAMPLES_PER_CLASS, len(files))\n",
    "                sampled = random.sample(files, n)\n",
    "            else:\n",
    "                sampled = files[:SAMPLES_PER_CLASS]\n",
    "\n",
    "            trial_paths.extend(sampled)\n",
    "\n",
    "        random.shuffle(trial_paths)\n",
    "\n",
    "    elif PLOT_SCOPE == \"all_in_one_class\":\n",
    "        if TARGET_CLASS not in classes:\n",
    "            raise ValueError(f\"TARGET_CLASS '{TARGET_CLASS}' not found. Available: {classes}\")\n",
    "        class_dir = DATASET_ROOT / TARGET_CLASS\n",
    "        trial_paths = list_npy_files(class_dir)\n",
    "        if not trial_paths:\n",
    "            raise RuntimeError(f\"No .npy files in class directory: {class_dir}\")\n",
    "\n",
    "    elif PLOT_SCOPE == \"single_file\":\n",
    "        if SINGLE_CLASS not in classes:\n",
    "            raise ValueError(f\"SINGLE_CLASS '{SINGLE_CLASS}' not found. Available: {classes}\")\n",
    "        class_dir = DATASET_ROOT / SINGLE_CLASS\n",
    "        files = list_npy_files(class_dir)\n",
    "        if not files:\n",
    "            raise RuntimeError(f\"No .npy files in class directory: {class_dir}\")\n",
    "        if not (0 <= SINGLE_FILE_INDEX < len(files)):\n",
    "            raise ValueError(f\"SINGLE_FILE_INDEX {SINGLE_FILE_INDEX} out of range (0..{len(files)-1})\")\n",
    "        trial_paths = [files[SINGLE_FILE_INDEX]]\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown PLOT_SCOPE: {PLOT_SCOPE}\")\n",
    "\n",
    "    print(f\"Total trials to plot: {len(trial_paths)}\")\n",
    "    plot_trials(trial_paths, RUN_OUTPUT_DIR, global_mean=global_mean, global_std=global_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "409dd27a-f1e0-447e-8a8b-c728f04b72e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET_ROOT : All_materials\n",
      "OUTPUT_DIR   : normalized_dataset_forplot/20251215_161803\n",
      "OFFSET range : 0..100 (inclusive)\n",
      "\n",
      "[1/2] computing global mean/std ...\n",
      "[global-stats] processed 200/250 files...\n",
      "[1/2] done.\n",
      "  mean shape: (16, 3) std shape: (16, 3)\n",
      "  std  min/max: 280.9351501464844 4830.4208984375\n",
      "\n",
      "[2/2] normalizing and saving dataset ...\n",
      "[save] total files to process: 250\n",
      "[save] processed 200/250 files...\n",
      "[save] done.\n",
      "\n",
      "All done. Normalized dataset saved under: normalized_dataset_forplot/20251215_161803\n"
     ]
    }
   ],
   "source": [
    "####データセット全体にて標準化し，保存し直す\n",
    "\n",
    "\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# ===================== 設定 =====================\n",
    "\n",
    "# 元データセット（クラスディレクトリ配下に .npy）\n",
    "DATASET_ROOT = Path(\"./All_materials/\")\n",
    "\n",
    "# 出力先（ここ配下に timestamp/クラス名/*.npy を作る）\n",
    "OUTPUT_ROOT = Path(\"./normalized_dataset_forplot\")\n",
    "\n",
    "# オフセット範囲（両端含む）\n",
    "OFFSET_T0 = 0\n",
    "OFFSET_T1 = 100\n",
    "\n",
    "# 数値安定化\n",
    "EPS = 1e-8\n",
    "\n",
    "# 保存 dtype（float32 推奨）\n",
    "SAVE_DTYPE = np.float32\n",
    "\n",
    "# 既に出力がある場合にスキップするか\n",
    "SKIP_IF_EXISTS = True\n",
    "\n",
    "\n",
    "# ===================== ユーティリティ =====================\n",
    "\n",
    "def list_classes(dataset_root: Path):\n",
    "    classes = [d.name for d in dataset_root.iterdir() if d.is_dir()]\n",
    "    classes.sort()\n",
    "    return classes\n",
    "\n",
    "def list_npy_files(class_dir: Path):\n",
    "    files = [p for p in class_dir.glob(\"*.npy\")]\n",
    "    files.sort()\n",
    "    return files\n",
    "\n",
    "def iter_all_npy_files(dataset_root: Path):\n",
    "    \"\"\"(class_name, path) を全て列挙\"\"\"\n",
    "    for cls in list_classes(dataset_root):\n",
    "        class_dir = dataset_root / cls\n",
    "        for p in list_npy_files(class_dir):\n",
    "            yield cls, p\n",
    "\n",
    "\n",
    "# ===================== 統計量（global mean/std）の推定 =====================\n",
    "\n",
    "def compute_global_stats_with_offset(dataset_root: Path,\n",
    "                                     offset_t0: int = 0,\n",
    "                                     offset_t1: int = 100,\n",
    "                                     eps: float = 1e-8):\n",
    "    \"\"\"\n",
    "    全 .npy を対象に:\n",
    "      1) 各 trial で t=offset_t0..offset_t1 の平均を引く（taxel×軸ごと）\n",
    "      2) オフセット除去後の global mean/std を taxel×軸ごとに推定\n",
    "\n",
    "    Returns:\n",
    "      mean: (n_taxels, 3)\n",
    "      std : (n_taxels, 3)\n",
    "    \"\"\"\n",
    "    paths = list(iter_all_npy_files(dataset_root))\n",
    "    if not paths:\n",
    "        raise RuntimeError(f\"No .npy files found under: {dataset_root}\")\n",
    "\n",
    "    sum_ = None\n",
    "    sumsq_ = None\n",
    "    total_count = 0\n",
    "    n_taxels_ref = None\n",
    "\n",
    "    for i, (_, path) in enumerate(paths):\n",
    "        data = np.load(path)  # (T, n_taxels, 3)\n",
    "\n",
    "        if data.ndim != 3 or data.shape[-1] != 3:\n",
    "            raise ValueError(f\"Invalid shape {data.shape} in {path} (expected (T, n_taxels, 3))\")\n",
    "\n",
    "        T, n_taxels, _ = data.shape\n",
    "        if T <= offset_t1:\n",
    "            raise ValueError(f\"T={T} too short for offset range {offset_t0}..{offset_t1} in {path}\")\n",
    "\n",
    "        if n_taxels_ref is None:\n",
    "            n_taxels_ref = n_taxels\n",
    "            sum_ = np.zeros((n_taxels_ref, 3), dtype=np.float64)\n",
    "            sumsq_ = np.zeros((n_taxels_ref, 3), dtype=np.float64)\n",
    "        elif n_taxels != n_taxels_ref:\n",
    "            raise ValueError(f\"n_taxels mismatch: {n_taxels} != {n_taxels_ref} in {path}\")\n",
    "\n",
    "        baseline = data[offset_t0:offset_t1 + 1].mean(axis=0, keepdims=True)  # (1, n_taxels, 3)\n",
    "        data0 = (data - baseline).astype(np.float64)  # (T, n_taxels, 3)\n",
    "\n",
    "        sum_ += data0.sum(axis=0)             # (n_taxels, 3)\n",
    "        sumsq_ += (data0 * data0).sum(axis=0) # (n_taxels, 3)\n",
    "        total_count += T\n",
    "\n",
    "        if (i + 1) % 200 == 0:\n",
    "            print(f\"[global-stats] processed {i+1}/{len(paths)} files...\")\n",
    "\n",
    "    mean = sum_ / total_count\n",
    "    var = (sumsq_ / total_count) - (mean * mean)\n",
    "    var = np.maximum(var, 0.0)\n",
    "    std = np.sqrt(var + eps)\n",
    "\n",
    "    return mean.astype(np.float32), std.astype(np.float32)\n",
    "\n",
    "\n",
    "def normalize_trial(data: np.ndarray, mean: np.ndarray, std: np.ndarray,\n",
    "                    offset_t0: int = 0, offset_t1: int = 100):\n",
    "    \"\"\"\n",
    "    data: (T, n_taxels, 3)\n",
    "    mean/std: (n_taxels, 3) （オフセット後のglobal統計）\n",
    "    \"\"\"\n",
    "    T, n_taxels, _ = data.shape\n",
    "    if T <= offset_t1:\n",
    "        raise ValueError(f\"T={T} too short for offset range {offset_t0}..{offset_t1}\")\n",
    "\n",
    "    baseline = data[offset_t0:offset_t1 + 1].mean(axis=0, keepdims=True)  # (1, n_taxels, 3)\n",
    "    data0 = data - baseline\n",
    "    data_norm = (data0 - mean[None, :, :]) / std[None, :, :]\n",
    "    return data_norm\n",
    "\n",
    "\n",
    "# ===================== 保存処理（同ディレクトリ構造） =====================\n",
    "\n",
    "def save_normalized_dataset(dataset_root: Path,\n",
    "                            out_root: Path,\n",
    "                            mean: np.ndarray,\n",
    "                            std: np.ndarray,\n",
    "                            offset_t0: int = 0,\n",
    "                            offset_t1: int = 100,\n",
    "                            save_dtype=np.float32,\n",
    "                            skip_if_exists: bool = True):\n",
    "    \"\"\"\n",
    "    DATASET_ROOT の構造:\n",
    "      dataset_root/\n",
    "        classA/*.npy\n",
    "        classB/*.npy\n",
    "        ...\n",
    "\n",
    "    OUTPUT の構造:\n",
    "      out_root/\n",
    "        classA/*.npy\n",
    "        classB/*.npy\n",
    "        ...\n",
    "\n",
    "    を維持して保存する。\n",
    "    \"\"\"\n",
    "    all_items = list(iter_all_npy_files(dataset_root))\n",
    "    print(f\"[save] total files to process: {len(all_items)}\")\n",
    "\n",
    "    for i, (cls, path) in enumerate(all_items):\n",
    "        rel_class_dir = cls\n",
    "        out_class_dir = out_root / rel_class_dir\n",
    "        out_class_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        out_path = out_class_dir / path.name\n",
    "        if skip_if_exists and out_path.exists():\n",
    "            continue\n",
    "\n",
    "        data = np.load(path)  # (T, n_taxels, 3)\n",
    "        data_norm = normalize_trial(data, mean, std, offset_t0, offset_t1).astype(save_dtype)\n",
    "\n",
    "        # 念のため shape チェック\n",
    "        if data_norm.shape != data.shape:\n",
    "            raise RuntimeError(f\"shape changed: {data.shape} -> {data_norm.shape} at {path}\")\n",
    "\n",
    "        np.save(out_path, data_norm)\n",
    "\n",
    "        if (i + 1) % 200 == 0:\n",
    "            print(f\"[save] processed {i+1}/{len(all_items)} files...\")\n",
    "\n",
    "    print(\"[save] done.\")\n",
    "\n",
    "\n",
    "# ===================== main =====================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # timestamp 付きの出力ディレクトリを作る\n",
    "    timestamp_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    RUN_OUT_DIR = OUTPUT_ROOT / timestamp_str\n",
    "    RUN_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"DATASET_ROOT : {DATASET_ROOT}\")\n",
    "    print(f\"OUTPUT_DIR   : {RUN_OUT_DIR}\")\n",
    "    print(f\"OFFSET range : {OFFSET_T0}..{OFFSET_T1} (inclusive)\")\n",
    "\n",
    "    # 1) global mean/std を計算（オフセット後）\n",
    "    print(\"\\n[1/2] computing global mean/std ...\")\n",
    "    global_mean, global_std = compute_global_stats_with_offset(\n",
    "        DATASET_ROOT, offset_t0=OFFSET_T0, offset_t1=OFFSET_T1, eps=EPS\n",
    "    )\n",
    "    print(\"[1/2] done.\")\n",
    "    print(\"  mean shape:\", global_mean.shape, \"std shape:\", global_std.shape)\n",
    "    print(\"  std  min/max:\", float(global_std.min()), float(global_std.max()))\n",
    "\n",
    "    # 統計量と設定を保存（再現用）\n",
    "    np.save(RUN_OUT_DIR / \"global_mean_taxel_axis.npy\", global_mean)\n",
    "    np.save(RUN_OUT_DIR / \"global_std_taxel_axis.npy\", global_std)\n",
    "    with open(RUN_OUT_DIR / \"normalization_info.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"DATASET_ROOT={DATASET_ROOT}\\n\")\n",
    "        f.write(f\"OFFSET_T0={OFFSET_T0}\\n\")\n",
    "        f.write(f\"OFFSET_T1={OFFSET_T1}\\n\")\n",
    "        f.write(f\"EPS={EPS}\\n\")\n",
    "        f.write(f\"SAVE_DTYPE={save_dtype_to_str(SAVE_DTYPE) if 'save_dtype_to_str' in globals() else str(SAVE_DTYPE)}\\n\")\n",
    "        f.write(f\"SKIP_IF_EXISTS={SKIP_IF_EXISTS}\\n\")\n",
    "\n",
    "    # 2) 全ファイルを正規化して同構造で保存\n",
    "    print(\"\\n[2/2] normalizing and saving dataset ...\")\n",
    "    save_normalized_dataset(\n",
    "        dataset_root=DATASET_ROOT,\n",
    "        out_root=RUN_OUT_DIR,\n",
    "        mean=global_mean,\n",
    "        std=global_std,\n",
    "        offset_t0=OFFSET_T0,\n",
    "        offset_t1=OFFSET_T1,\n",
    "        save_dtype=SAVE_DTYPE,\n",
    "        skip_if_exists=SKIP_IF_EXISTS\n",
    "    )\n",
    "    print(f\"\\nAll done. Normalized dataset saved under: {RUN_OUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0dbfb0-fa5e-4ae9-b31c-4077bb82e496",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
