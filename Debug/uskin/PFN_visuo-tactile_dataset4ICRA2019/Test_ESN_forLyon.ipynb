{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e6a0c05-f70f-44bd-b300-6587a012f15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random \n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from scipy import signal\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, random_split, Subset\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "label_encoder = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e08e914-0074-45d8-84ef-de89ffa76e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnekodaisuki169\u001b[0m (\u001b[33mdoctor_thesis_material\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/sota/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "# WandBの設定\n",
    "WANDB_API_KEY = \"2d996a98ef8dddefa91d675f85b5efd96fb911ae\"  # あなたのWandB APIキーをここに入力してください\n",
    "\n",
    "wandb.login(key = WANDB_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1302633-82bd-4ec5-9664-f21f63001191",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TactileSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    触覚センサ npy データ用 Dataset クラス\n",
    "\n",
    "    - 入力ファイル: shape (T, n_taxels, 3)\n",
    "    - seq_start〜seq_end ステップを切り出し\n",
    "    - (seq_len, n_taxels, 3) -> (seq_len, n_taxels*3) にフラット化\n",
    "    - ESN 用に (1, seq_len, feature_dim) に reshape し、バッチ化で\n",
    "      (batch_size, 1, seq_len, feature_dim) になるようにする\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, dataset_params):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir: クラスディレクトリが並んでいるルートパス\n",
    "                      例: dataset_root/\n",
    "                            paper_A4/\n",
    "                              xxx.npy\n",
    "                            cloth/\n",
    "                              yyy.npy\n",
    "            dataset_params:\n",
    "                - \"seq_start\": 切り出し開始インデックス（含む）\n",
    "                - \"seq_end\"  : 切り出し終了インデックス（含まない）\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.seq_start = int(dataset_params[\"seq_start\"])\n",
    "        self.seq_end   = int(dataset_params[\"seq_end\"])\n",
    "        self.dtype     = torch.float32  # ← カンマ削除\n",
    "\n",
    "        # クラスディレクトリを列挙\n",
    "        self.class_names = sorted(\n",
    "            [d.name for d in self.root_dir.iterdir() if d.is_dir()]\n",
    "        )\n",
    "        if not self.class_names:\n",
    "            raise RuntimeError(f\"No class directories under {self.root_dir}\")\n",
    "\n",
    "        self.class_to_idx = {c: i for i, c in enumerate(self.class_names)}\n",
    "        self.num_classes  = len(self.class_names)  # ★ one-hot 用\n",
    "\n",
    "        # 各 npy ファイルのパスとラベルを列挙\n",
    "        self.samples = []  # list of (path, label_idx)\n",
    "        for class_name in self.class_names:\n",
    "            class_dir = self.root_dir / class_name\n",
    "            for npy_path in sorted(class_dir.glob(\"*.npy\")):\n",
    "                self.samples.append((npy_path, self.class_to_idx[class_name]))\n",
    "\n",
    "        if not self.samples:\n",
    "            raise RuntimeError(f\"No .npy files found under {self.root_dir}\")\n",
    "\n",
    "        # シーケンス長を確認 (最初のサンプルでチェック)\n",
    "        arr0 = np.load(self.samples[0][0])\n",
    "        T, n_taxels, axes = arr0.shape\n",
    "        assert axes == 3, f\"Last dimension must be 3 (x,y,z), got {axes}\"\n",
    "        if self.seq_end > T:\n",
    "            raise ValueError(\n",
    "                f\"seq_end({self.seq_end}) is larger than T({T}). \"\n",
    "                \"Please adjust seq_start/seq_end.\"\n",
    "            )\n",
    "\n",
    "        self.original_T = T\n",
    "        self.n_taxels   = n_taxels\n",
    "        self.seq_len    = self.seq_end - self.seq_start\n",
    "        self.feature_dim = self.n_taxels * 3  # x, y, z をまとめた次元\n",
    "\n",
    "        print(\"=== TactileSequenceDataset initialized ===\")\n",
    "        print(f\"root_dir     : {self.root_dir}\")\n",
    "        print(f\"num_classes  : {self.num_classes}\")\n",
    "        print(f\"num_samples  : {len(self.samples)}\")\n",
    "        print(f\"original T   : {self.original_T}\")\n",
    "        print(f\"seq range    : [{self.seq_start}, {self.seq_end}) -> seq_len={self.seq_len}\")\n",
    "        print(f\"n_taxels     : {self.n_taxels}\")\n",
    "        print(f\"feature_dim  : {self.feature_dim} (= n_taxels * 3)\")\n",
    "        print(\"class_to_idx : \", self.class_to_idx)\n",
    "        print(\"=========================================\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        npy_path, label_idx = self.samples[idx]\n",
    "        arr = np.load(npy_path)  # shape (T, n_taxels, 3)\n",
    "\n",
    "        # 指定区間を切り出し\n",
    "        seq = arr[self.seq_start:self.seq_end]  # (seq_len, n_taxels, 3)\n",
    "\n",
    "        # (seq_len, n_taxels, 3) -> (seq_len, n_taxels * 3)\n",
    "        seq = seq.reshape(self.seq_len, self.feature_dim)\n",
    "\n",
    "        # torch.Tensor に変換 (seq_len, feature_dim)\n",
    "        x = torch.from_numpy(seq).to(self.dtype)\n",
    "\n",
    "        # ESN が期待する (batch, channel_size, seq_len, input_size) に合わせて\n",
    "        # channel_size = 1 として (1, seq_len, feature_dim) に変換\n",
    "        x = x.unsqueeze(0)  # (1, seq_len, feature_dim)\n",
    "\n",
    "        # ラベルを one-hot ベクトルに変換\n",
    "        label_idx_tensor = torch.tensor(label_idx, dtype=torch.long)\n",
    "        y = F.one_hot(label_idx_tensor, num_classes=self.num_classes).to(self.dtype)\n",
    "        # y.shape -> (num_classes,)  バッチ化後: (batch_size, num_classes)\n",
    "\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0882e39-81e6-4e0b-b43e-dc69034082df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EchoStateNetwork(nn.Module):\n",
    "    def __init__(self, model_params, dataset_params):\n",
    "        super(EchoStateNetwork, self).__init__()\n",
    "        \n",
    "        self.reservoir_size = int(model_params[\"reservoir_size\"])\n",
    "        self.reservoir_weights_scale = float(model_params[\"reservoir_weights_scale\"])\n",
    "        \n",
    "        self.input_size = int(model_params[\"input_size\"])\n",
    "        self.channel_size = int(model_params[\"channel_size\"])\n",
    "        self.input_weights_scale = float(model_params[\"input_weights_scale\"])\n",
    "        self.spectral_radius = float(model_params[\"spectral_radius\"])\n",
    "        self.density = float(model_params[\"reservoir_density\"])\n",
    "        self.leak_rate = float(model_params[\"leak_rate\"])\n",
    "        \n",
    "        self.sequence_length = int(int(dataset_params[\"sequence_length\"]) / int(dataset_params[\"slicing_size\"]))\n",
    "        # self.sequence_length = int(dataset_params[\"seq_end\"]) - int(dataset_params[\"seq_start\"])\n",
    "\n",
    "        # リザバー結合行列 (ランダムに初期化)\n",
    "        self.register_parameter(\"reservoir_weights\", torch.nn.Parameter(torch.empty((self.reservoir_size, self.reservoir_size)).uniform_(-self.reservoir_weights_scale, self.reservoir_weights_scale).to(device), requires_grad=False))\n",
    "        \n",
    "        # リザバー入力行列 (ランダムに初期化)\n",
    "        self.register_parameter(\"input_weights\", torch.nn.Parameter(torch.empty((self.reservoir_size, self.input_size * self.channel_size)).uniform_(-self.input_weights_scale, self.input_weights_scale).to(device), requires_grad=False))\n",
    "        \n",
    "        \n",
    "\n",
    "        #リザバー結合のスパース処理\n",
    "        self.reservoir_weights_mask = torch.empty((self.reservoir_size, self.reservoir_size)).uniform_(0, 1)\n",
    "        self.reservoir_weights_mask = torch.where(self.reservoir_weights_mask < self.density, torch.tensor(1), torch.tensor(0)).to(device)\n",
    "        self.reservoir_weights *= self.reservoir_weights_mask\n",
    "        \n",
    "        #スペクトル半径の処理\n",
    "        _, singular_values, _ = torch.svd(self.reservoir_weights)\n",
    "        rho_reservoir_weights = torch.max(singular_values).item()\n",
    "        self.reservoir_weights *= self.spectral_radius / rho_reservoir_weights\n",
    "        \n",
    "       #最終時刻における，リザバー状態ベクトル\n",
    "        self.last_reservoir_state_matrix = torch.zeros(self.channel_size, self.reservoir_size).to(device)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        batch_size = x.size(0)\n",
    "        sequence_length = x.size(2)\n",
    "        # x_seq = x.view(batch_size, self.channel_size, self.input_size, sequence_length).to(device)\n",
    "\n",
    "        # if self.last_reservoir_state_matrix is None or self.last_reservoir_state_matrix.size(0) != batch_size:\n",
    "        #     self.last_reservoir_state_matrix = torch.zeros(\n",
    "        #         batch_size, self.channel_size, self.reservoir_size, device=device\n",
    "        #     )\n",
    "        \n",
    "        # 各シーケンスのバッチに対してリザバー状態を初期化\n",
    "        self.reservoir_state_matrix = torch.zeros(batch_size, self.channel_size,  sequence_length, self.reservoir_size).to(device)\n",
    "\n",
    "        self.last_reservoir_state_matrix = torch.zeros(\n",
    "                batch_size, self.channel_size, self.reservoir_size, device=device\n",
    "            )\n",
    "        \n",
    "        for t in range(sequence_length):\n",
    "            input_at_t = torch.matmul(x[:, :, t, :].view(batch_size, -1), self.input_weights.t())\n",
    "            input_at_t = input_at_t.unsqueeze(1)\n",
    "\n",
    "            if t == 0:\n",
    "                state_update = torch.matmul(self.last_reservoir_state_matrix, self.reservoir_weights)\n",
    "            else:\n",
    "                # print(\"11111\")\n",
    "                state_update = torch.matmul(self.reservoir_state_matrix[:, :, t-1, :], self.reservoir_weights)\n",
    "                # print(f\"state_update.shape {state_update.shape}\")\n",
    "            self.reservoir_state_matrix[:, :, t, :] = self.leak_rate * torch.tanh(input_at_t + state_update) + \\\n",
    "                                                    (1 - self.leak_rate) * self.reservoir_state_matrix[:, :, t-1, :]\n",
    "\n",
    "        \n",
    "\n",
    "        self.last_reservoir_state_matrix = self.reservoir_state_matrix[:, :, -1, :]\n",
    "        return self.reservoir_state_matrix\n",
    "\n",
    "    def reset_hidden_state(self):\n",
    "        self.last_reservoir_state_matrix = torch.zeros(self.channel_size, self.reservoir_size, device=device)\n",
    "        # pass\n",
    "        # print(\"内部状態がリセットされました\")\n",
    "    \n",
    "#リードアウト層\n",
    "class ReadOut(nn.Module):\n",
    "    def __init__(self, model_params, dataset_params):\n",
    "        super(ReadOut, self).__init__()\n",
    "        # self.reservoir_state_matrix_size = int(model_params[\"reservoir_size\"]) + int(model_params[\"input_size\"]) + 1\n",
    "        self.reservoir_state_matrix_size = int(model_params[\"reservoir_size\"])\n",
    "        self.output_size = int(model_params[\"ReadOut_output_size\"])\n",
    "        self.batch_training = model_params[\"Batch_Training\"]\n",
    "        self.channel_size = int(model_params[\"channel_size\"])\n",
    "        \n",
    "        self.sequence_length = int(int(dataset_params[\"sequence_length\"]) / int(dataset_params[\"slicing_size\"]))\n",
    "        # self.sequence_length = int(dataset_params[\"seq_end\"]) - int(dataset_params[\"seq_start\"])\n",
    "        \n",
    "        self.readout_dense = nn.Linear(self.reservoir_state_matrix_size, self.output_size, bias=False)\n",
    "        \n",
    "        #線形回帰におけるバッチ学習を行うならば，リードアウト層を最急降下法による学習対象にしない\n",
    "        if self.batch_training == True:\n",
    "            self.readout_dense.weight.requires_grad = False\n",
    "        else:\n",
    "            None\n",
    "            \n",
    "        nn.init.xavier_uniform_(self.readout_dense.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, channel_size, input_size, sequence_length]\n",
    "        # batch_size, channel_size, input_size, sequence_length = x.size()\n",
    "        \n",
    "        # Reshape x to apply the dense layer to each time step and channel independently\n",
    "        # New shape: [batch_size * channel_size * sequence_length, input_size]\n",
    "        # x_reshaped = x.permute(0, 1, 3, 2).contiguous().view(-1, input_size)\n",
    "        \n",
    "        # Apply the dense layer\n",
    "        # Output shape: [batch_size * channel_size * sequence_length, output_size]\n",
    "        # print(f\"readout x shape{x.shape}\")\n",
    "        output = self.readout_dense(x)\n",
    "        \n",
    "        # Reshape the output back to the original form\n",
    "        # Output shape: [batch_size, channel_size, output_size, sequence_length]\n",
    "        # output = output.view(batch_size, channel_size, sequence_length, -1).permute(0, 1, 3, 2).contiguous()\n",
    "        return output\n",
    "    \n",
    "    # リッジ回帰によるリードアウトの導出\n",
    "    @staticmethod\n",
    "    def ridge_regression(X, Y, alpha):\n",
    "        # データ行列 X の形状を取得\n",
    "        n, p = X.shape\n",
    "\n",
    "        # 正則化項の行列を作成\n",
    "        ridge_matrix = (alpha * torch.eye(n)).float().to(device)\n",
    "        X = X.float()\n",
    "        Y = Y.float()\n",
    "\n",
    "        # リッジ回帰の係数を計算\n",
    "        coefficients = torch.linalg.solve(torch.matmul(X, X.T) + ridge_matrix, torch.matmul(X, Y.T)).T\n",
    "\n",
    "        return coefficients\n",
    "\n",
    "    @staticmethod\n",
    "    def ridge_regression_update(outputs, targets, model, alpha=0):\n",
    "        with torch.no_grad():\n",
    "            # リッジ回帰を用いて重みを求める\n",
    "            # モデルのパラメータを更新\n",
    "            new_weights = ReadOut.ridge_regression(outputs.squeeze(), targets.squeeze(), alpha)\n",
    "\n",
    "            # モデルのパラメータを更新\n",
    "            model.ReadOut.readout_dense.weight.copy_(new_weights)\n",
    "        \n",
    "        return None\n",
    "    #それぞれのモデルパラメータ候補を辞書に格納する\n",
    "    @staticmethod\n",
    "    def model_params_candinate(model_params):\n",
    "        model_params_combinations = list(itertools.product(*model_params.values()))\n",
    "        param_dicts = [dict(zip(model_params.keys(), combination)) for combination in model_params_combinations]\n",
    "        \n",
    "        return param_dicts\n",
    "\n",
    "    #モデル構造を辞書型に格納\n",
    "    @staticmethod\n",
    "    def model_sturcture_dict(model):\n",
    "        layers_dict = {}\n",
    "        for name, module in model.named_modules():\n",
    "            layers_dict[name] = {\n",
    "                'type': type(module).__name__,\n",
    "                'parameters': {p: getattr(module, p) for p in module.__dict__ if not p.startswith('_')}\n",
    "            }\n",
    "        \n",
    "        #モデル名と初期の引数は削除\n",
    "        del(layers_dict[''])\n",
    "        \n",
    "        return layers_dict\n",
    "\n",
    "class ESN(nn.Module):\n",
    "    def __init__(self, model_params, training_params, dataset_params):\n",
    "        super(ESN, self).__init__()\n",
    "        self.ESN = EchoStateNetwork(model_params, dataset_params)\n",
    "        self.ReadOut = ReadOut(model_params, dataset_params)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.Reservoir_State_Matrix = self.ESN(x)\n",
    "        # print(f\"resever state matrix {self.Reservoir_State_Matrix.shape}\")\n",
    "        self.ReadOut_Reservoir = self.ReadOut(self.Reservoir_State_Matrix)\n",
    "        # print(\"重み:\", self.ReadOut.readout_dense.weight)\n",
    "        # print(self.ReadOut_Reservoir)\n",
    "\n",
    "        #channle_size次元はいらないので，減らす\n",
    "        self.ReadOut_Reservoir = self.ReadOut_Reservoir.squeeze(1)\n",
    "        # print(f\"output matrix {self.ReadOut_Reservoir.shape}\")\n",
    "\n",
    "        return self.ReadOut_Reservoir\n",
    "\n",
    "    def reset_hidden_state(self):\n",
    "        self.ESN.reset_hidden_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92edc84a-9505-4e3d-993d-ca7aaa973e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K分割交差検証とデータローダーの生成\n",
    "def create_cross_validation_dataloaders(dataset, dataset_params, traing_params):\n",
    "    n_splits = int(traing_params[\"n_splits\"])\n",
    "    batch_size = int(dataset_params[\"batch_size\"])\n",
    "    augmentation_factor = int(dataset_params[\"augmentation_factor\"])\n",
    "    \n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    split_datasets = []\n",
    "\n",
    "    for train_idx, val_idx in kf.split(range(len(dataset))):\n",
    "        train_subset = Subset(dataset, train_idx)\n",
    "        val_subset = Subset(dataset, val_idx)\n",
    "        \n",
    "        # 学習用サブセットにのみ拡張を適用\n",
    "        # train_subset = AugmentedDataset(train_subset, dataset_params)\n",
    "\n",
    "        train_loader = DataLoader(train_subset, len(train_subset), shuffle=True)\n",
    "        val_loader = DataLoader(val_subset, len(val_subset), shuffle=False)\n",
    "\n",
    "        split_datasets.append((train_loader, val_loader))\n",
    "    \n",
    "    return split_datasets\n",
    "\n",
    "def prepare_datasets(dataset_params, traing_params, data_dir):\n",
    "    testdata_ratio = float(traing_params[\"testdata_ratio\"])\n",
    "    batch_size = int(dataset_params[\"batch_size\"])\n",
    "    \n",
    "    # データセットの準備\n",
    "    dataset = TactileSequenceDataset(data_dir, dataset_params)  # 拡張はここでは適用しない\n",
    "\n",
    "    # データセットを学習用、テスト用に分割する\n",
    "    test_size = int(len(dataset) * testdata_ratio)\n",
    "    train_size = len(dataset) - test_size\n",
    "    crossval_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    # 交差検証のためのデータローダーを準備\n",
    "    cross_val_loaders = create_cross_validation_dataloaders(crossval_dataset, dataset_params, traing_params)\n",
    "\n",
    "    # テストデータローダーの準備\n",
    "    test_loader = DataLoader(test_dataset, batch_size, shuffle=False)\n",
    "\n",
    "    return cross_val_loaders, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5892294-5337-4f93-9584-310490ac3c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, train_loader, dataset_params, model_params, traing_params):\n",
    "    model.train()  # Set model to training mode\n",
    "    num_epochs = int(traing_params[\"num_epochs\"])   \n",
    "    reservoir_size = int(model_params[\"reservoir_size\"])   \n",
    "    input_size = int(model_params[\"input_size\"])\n",
    "    ReadOut_output_size = int(model_params[\"ReadOut_output_size\"])\n",
    "    sequence_length = int(int(dataset_params[\"sequence_length\"]) / int(dataset_params[\"slicing_size\"]))\n",
    "    batch_training = model_params[\"Batch_Training\"]\n",
    "    Regularization_L2 = model_params[\"Regularization_L2\"]\n",
    "\n",
    "    # ★ 時間方向ストライド（無指定なら 1）\n",
    "    time_stride = int(dataset_params.get(\"time_stride\", 1))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.float().to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # ESN & Readout\n",
    "            outputs_ESN = model.ESN(inputs)          # 例: [B, 1, T, H] or [B, T, H]\n",
    "            outputs = model.ReadOut(outputs_ESN)     # 例: [B, 1, T, C] or [B, T, C]\n",
    "            outputs = outputs.squeeze()              # 期待形状: [B, T, C]\n",
    "\n",
    "            # ===== 形状を統一: reservoir_states -> [B, T, H] =====\n",
    "            if outputs_ESN.dim() == 4:\n",
    "                # [B, 1, T, H] を想定\n",
    "                B, ch, T_full, H = outputs_ESN.shape\n",
    "                reservoir_states = outputs_ESN[:, 0]        # [B, T_full, H]\n",
    "            elif outputs_ESN.dim() == 3:\n",
    "                # [B, T, H]\n",
    "                B, T_full, H = outputs_ESN.shape\n",
    "                reservoir_states = outputs_ESN\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected ESN output shape: {outputs_ESN.shape}\")\n",
    "\n",
    "            if outputs.dim() != 3:\n",
    "                raise ValueError(f\"Unexpected ReadOut output shape: {outputs.shape}\")\n",
    "\n",
    "            _, T_out, C = outputs.shape\n",
    "            assert T_out == T_full, \"ESN と ReadOut の時間長が一致していません\"\n",
    "\n",
    "            # ===== 時間方向のストライドサンプリング =====\n",
    "            if time_stride > 1:\n",
    "                reservoir_states = reservoir_states[:, ::time_stride, :]  # [B, T_eff, H]\n",
    "                outputs = outputs[:, ::time_stride, :]                    # [B, T_eff, C]\n",
    "\n",
    "            B_eff, T_eff, H_eff = reservoir_states.shape\n",
    "            _, T_eff_out, C = outputs.shape\n",
    "            assert T_eff == T_eff_out\n",
    "\n",
    "            # ===== flatten の整合性を取る =====\n",
    "            # リザバー: [B, T_eff, H] -> [H, B*T_eff]\n",
    "            outputs_ESN_flatten = (\n",
    "                reservoir_states.permute(2, 0, 1).contiguous().view(H_eff, -1)\n",
    "            )\n",
    "\n",
    "            # 出力: [B, T_eff, C] -> [C, B*T_eff]\n",
    "            outputs_flatten = (\n",
    "                outputs.permute(2, 0, 1).contiguous().view(C, -1)\n",
    "            )\n",
    "\n",
    "            # ラベル: [B, C] を T_eff 回繰り返し → [B, T_eff, C] → [C, B*T_eff]\n",
    "            labels_rep = labels.unsqueeze(1).repeat(1, T_eff, 1)          # [B, T_eff, C]\n",
    "            labels_flatten = (\n",
    "                labels_rep.permute(2, 0, 1).contiguous().view(C, -1)      # [C, B*T_eff]\n",
    "            )\n",
    "\n",
    "            loss = criterion(outputs_flatten, labels_flatten)\n",
    "            \n",
    "            if batch_training == True:\n",
    "                # ★ 時間ストライド後のリザバー状態でリッジ更新\n",
    "                model.ReadOut.ridge_regression_update(\n",
    "                    outputs_ESN_flatten, labels_flatten, model, Regularization_L2\n",
    "                )\n",
    "            else:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() \n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
    "        wandb.log({\"loss\": epoch_loss})\n",
    "    \n",
    "    print('Training complete')\n",
    "\n",
    "\n",
    "\n",
    "def validate_model(model, val_loader, dataset_params, model_params):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_running_loss = 0.0\n",
    "\n",
    "    reservoir_size = int(model_params[\"reservoir_size\"])   \n",
    "    input_size = int(model_params[\"input_size\"])\n",
    "    ReadOut_output_size = int(model_params[\"ReadOut_output_size\"])\n",
    "    sequence_length = int(int(dataset_params[\"sequence_length\"]) / int(dataset_params[\"slicing_size\"]))\n",
    "\n",
    "    # ★ 学習時と同じ time_stride を使う\n",
    "    time_stride = int(dataset_params.get(\"time_stride\", 1))\n",
    "\n",
    "    last_outputs_flatten = None\n",
    "    last_labels_flatten = None\n",
    "    last_T_eff = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.float().to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs_ESN = model.ESN(inputs)              # [B, 1, T, H] or [B, T, H]\n",
    "            outputs = model.ReadOut(outputs_ESN).squeeze(1)  # [B, T, C]\n",
    "\n",
    "            # ===== 形状を統一: reservoir_states -> [B, T, H] =====\n",
    "            if outputs_ESN.dim() == 4:\n",
    "                B, ch, T_full, H = outputs_ESN.shape\n",
    "                reservoir_states = outputs_ESN[:, 0]        # [B, T_full, H]\n",
    "            elif outputs_ESN.dim() == 3:\n",
    "                B, T_full, H = outputs_ESN.shape\n",
    "                reservoir_states = outputs_ESN\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected ESN output shape: {outputs_ESN.shape}\")\n",
    "\n",
    "            if outputs.dim() != 3:\n",
    "                raise ValueError(f\"Unexpected ReadOut output shape: {outputs.shape}\")\n",
    "\n",
    "            _, T_out, C = outputs.shape\n",
    "            assert T_out == T_full\n",
    "\n",
    "            # ===== 時間方向のストライドサンプリング =====\n",
    "            if time_stride > 1:\n",
    "                reservoir_states = reservoir_states[:, ::time_stride, :]\n",
    "                outputs = outputs[:, ::time_stride, :]\n",
    "\n",
    "            B_eff, T_eff, H_eff = reservoir_states.shape\n",
    "            _, T_eff_out, C = outputs.shape\n",
    "            assert T_eff == T_eff_out\n",
    "            last_T_eff = T_eff  # compute_accuracy 用に覚えておく\n",
    "\n",
    "            # ===== flatten =====\n",
    "            outputs_flatten = (\n",
    "                outputs.permute(2, 0, 1).contiguous().view(C, -1)\n",
    "            )\n",
    "            labels_rep = labels.unsqueeze(1).repeat(1, T_eff, 1)\n",
    "            labels_flatten = (\n",
    "                labels_rep.permute(2, 0, 1).contiguous().view(C, -1)\n",
    "            )\n",
    "\n",
    "            loss = criterion(outputs_flatten, labels_flatten)\n",
    "            val_running_loss += loss.item() * B_eff\n",
    "\n",
    "            last_outputs_flatten = outputs_flatten\n",
    "            last_labels_flatten = labels_flatten\n",
    "\n",
    "    val_loss = val_running_loss / len(val_loader.dataset)\n",
    "\n",
    "    # ★ compute_accuracy にはストライド後の T_eff を渡す\n",
    "    if last_outputs_flatten is not None and last_labels_flatten is not None and last_T_eff is not None:\n",
    "        val_accuracy = compute_accuracy(last_outputs_flatten, last_labels_flatten, last_T_eff)\n",
    "    else:\n",
    "        val_accuracy = 0.0\n",
    "\n",
    "    print(f\"Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "    print(f'Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}')\n",
    "    wandb.log({\"val_accuracy\": val_accuracy, \"val_loss\": val_loss})\n",
    "\n",
    "    \n",
    "def test_model(model, val_loader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_running_loss = 0.0\n",
    "    val_running_corrects = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.float().to(device)\n",
    "            labels = labels.squeeze().to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "#             _, label_preds = torch.max(labels, 1)\n",
    "            label_preds = labels\n",
    "\n",
    "            val_running_loss += loss.item() * inputs.size(0)\n",
    "            print(loss.item())\n",
    "            val_running_corrects += torch.sum(preds == label_preds)\n",
    "\n",
    "    val_loss = val_running_loss / len(val_loader.dataset)\n",
    "    val_accuracy = val_running_corrects.double() / len(val_loader.dataset)\n",
    "    \n",
    "    print(f'Test Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}')\n",
    "    wandb.log({\"test_accuracy\": val_accuracy, \"test_loss\": val_loss})\n",
    "\n",
    "def compute_accuracy(model_output, target, n_taus):\n",
    "    # モデルの出力と教師ラベルを各データに分割\n",
    "    split_model_output = torch.split(model_output.squeeze(), n_taus, dim=-1)\n",
    "    split_target = torch.split(target.squeeze(), n_taus, dim=-1)\n",
    "    # print(\"aaaa\")\n",
    "    # print(split_model_output[0].shape)\n",
    "    # print(split_target[0].shape)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for pred, true_label in zip(split_model_output, split_target):\n",
    "        # 最も確率が高いラベルを予測ラベルとして取得\n",
    "        # print(pred.shape)\n",
    "        # print(pred)\n",
    "        # print(true_label.shape)\n",
    "        # print(true_label)\n",
    "        count_ones = (true_label == 1).sum().item()\n",
    "        # print(count_ones)\n",
    "        histgram_predict = torch.bincount(torch.max(pred, 0)[1])\n",
    "        _, predicted = torch.max(histgram_predict, 0)\n",
    "    \n",
    "        histgram_true_label_idx = torch.bincount(torch.max(true_label, 0)[1])\n",
    "        _, true_label_idx = torch.max(histgram_true_label_idx, 0)\n",
    "\n",
    "        # 正解数をカウント\n",
    "        correct += (predicted == true_label_idx).sum().item()\n",
    "        total += 1\n",
    "\n",
    "    # 精度を算出\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "def model_params_candinate(model_params):\n",
    "    model_params_combinations = list(itertools.product(*model_params.values()))\n",
    "    param_dicts = [dict(zip(model_params.keys(), combination)) for combination in model_params_combinations]\n",
    "    return param_dicts\n",
    "\n",
    "# モデル構造を辞書型に格納\n",
    "def model_sturcture_dict(model):\n",
    "    layers_dict = {}\n",
    "    for name, module in model.named_modules():\n",
    "        layers_dict[name] = {\n",
    "            'type': type(module).__name__,\n",
    "            'parameters': {p: getattr(module, p) for p in module.__dict__ if not p.startswith('_')}\n",
    "        }\n",
    "    # モデル名と初期の引数は削除\n",
    "    del(layers_dict[''])\n",
    "    return layers_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d775fe13-6dbc-47b8-a21b-4e1cb5d48db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TactileSequenceDataset initialized ===\n",
      "root_dir     : All_materials\n",
      "num_classes  : 25\n",
      "num_samples  : 250\n",
      "original T   : 1255\n",
      "seq range    : [400, 1200) -> seq_len=800\n",
      "n_taxels     : 16\n",
      "feature_dim  : 48 (= n_taxels * 3)\n",
      "class_to_idx :  {'01_table_cover': 0, '02_fur_scarf': 1, '03_washing_towel': 2, '04_carpet1': 3, '05_bubble_wrap': 4, '06_fleece_scarf': 5, '07_knit_hat1': 6, '08_body_towel1': 7, '09_body_towel2': 8, '10_carpet2': 9, '11_work_gloves': 10, '12_knit_hat2': 11, '13_toilet_mat1': 12, '14_floor_mat': 13, '15_sponge1': 14, '16_printed_tatami': 15, '17_cushion1': 16, '18_mop': 17, '19_toilet_mat2': 18, '20_fleece_sock': 19, '21_cushion': 20, '22_carpet3': 21, '23_fleece_mat': 22, '24_carpet4': 23, '25_sponge2': 24}\n",
      "=========================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/sota/Doctor_Thesis_Code/Debug/uskin/PFN_visuo-tactile_dataset4ICRA2019/wandb/run-20251215_234239-r1po1bxt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/doctor_thesis_material/uskin_test_ESN/runs/r1po1bxt' target=\"_blank\">stilted-tree-154</a></strong> to <a href='https://wandb.ai/doctor_thesis_material/uskin_test_ESN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/doctor_thesis_material/uskin_test_ESN' target=\"_blank\">https://wandb.ai/doctor_thesis_material/uskin_test_ESN</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/doctor_thesis_material/uskin_test_ESN/runs/r1po1bxt' target=\"_blank\">https://wandb.ai/doctor_thesis_material/uskin_test_ESN/runs/r1po1bxt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "Epoch 1/1, Loss: 0.0095\n",
      "Training complete\n",
      "Accuracy: 66.00%\n",
      "Validation Loss: 0.0271, Accuracy: 0.6600\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Epoch 1/1, Loss: 0.0091\n",
      "Training complete\n",
      "Accuracy: 86.00%\n",
      "Validation Loss: 0.0242, Accuracy: 0.8600\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Epoch 1/1, Loss: 0.0103\n",
      "Training complete\n",
      "Accuracy: 82.00%\n",
      "Validation Loss: 0.0238, Accuracy: 0.8200\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Epoch 1/1, Loss: 0.0084\n",
      "Training complete\n",
      "Accuracy: 70.00%\n",
      "Validation Loss: 0.0251, Accuracy: 0.7000\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Epoch 1/1, Loss: 0.0152\n",
      "Training complete\n",
      "Accuracy: 88.00%\n",
      "Validation Loss: 0.0235, Accuracy: 0.8800\n",
      "CrossVaridation Finished\n",
      "--------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>▂▂▃▁█</td></tr><tr><td>val_accuracy</td><td>▁▇▆▂█</td></tr><tr><td>val_loss</td><td>█▂▁▄▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.01522</td></tr><tr><td>val_accuracy</td><td>0.88</td></tr><tr><td>val_loss</td><td>0.02354</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stilted-tree-154</strong> at: <a href='https://wandb.ai/doctor_thesis_material/uskin_test_ESN/runs/r1po1bxt' target=\"_blank\">https://wandb.ai/doctor_thesis_material/uskin_test_ESN/runs/r1po1bxt</a><br/> View project at: <a href='https://wandb.ai/doctor_thesis_material/uskin_test_ESN' target=\"_blank\">https://wandb.ai/doctor_thesis_material/uskin_test_ESN</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251215_234239-r1po1bxt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#各種のパラメータ設定\n",
    "dataset_params = {\"seq_start\" : 400, \"seq_end\" : 1200, \"sequence_length\": 800, \"slicing_size\" : 1, \"augmentation_factor\": 0, \"batch_size\" : 32, \"Onehot_Encoding\" : None, \"augmentation_mu\" : 0, \"augmentation_sigma\" : 0, \"augmentation_shift\" : 1, \"time_stride\":1}\n",
    "model_params = {\"reservoir_size\" : [300],\"input_size\" : [48], \"channel_size\" : [1],  \"reservoir_weights_scale\" : [1], \"input_weights_scale\" : [10000], \"spectral_radius\" : [1.5],\"reservoir_density\" : [0.02], \"leak_rate\" : [0.1], \"Batch_Training\" : [True], \"ReadOut_output_size\" : [25], \"Regularization_L2\" : [0.01]}\n",
    "training_params = {\"num_epochs\" : 1, \"learning_rate\" : 0.01, \"weight_decay\" : 1e-2, \"testdata_ratio\" : 0, \"n_splits\" : 5}\n",
    "\n",
    "#それぞれのモデルパラメータ候補を辞書に格納する\n",
    "model_params = model_params_candinate(model_params)\n",
    "\n",
    "#学習データセットの設定\n",
    "data_dir=\"./All_materials/\"\n",
    "cross_val_loaders, test_loader = prepare_datasets(dataset_params, training_params, data_dir)\n",
    "\n",
    "# wandb.init(project=\"uskin_test_ESN\", config=config_dictionary)\n",
    "\n",
    "#モデルパラメータの候補ごとに，総当たりしてパラメータを探索する\n",
    "\n",
    "for each_model_params in model_params:\n",
    "    \n",
    "#     model = LSTMModel(each_model_params).to(device)\n",
    "    model = ESN(each_model_params, training_params, dataset_params).to(device)\n",
    "    model_sturcture = model_sturcture_dict(model)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = float(training_params[\"learning_rate\"]), weight_decay = float(training_params[\"weight_decay\"]))\n",
    "    \n",
    "    config_dictionary = {\n",
    "    \"dataset\": data_dir,\n",
    "    \"dataset_params\" : dataset_params,\n",
    "    \"architecture\": model.__class__.__name__,\n",
    "    \"model_params\" : each_model_params,\n",
    "    \"model_sturcture\" : model_sturcture,\n",
    "    \"traing_params\" : training_params,\n",
    "    \"criterion\" : str(criterion),\n",
    "    \"optimizer\" : str(optimizer),\n",
    "    }\n",
    "\n",
    "    wandb.init(project=\"uskin_test_ESN\", config=config_dictionary)\n",
    "\n",
    "    # 4. k-fold交差検証のループ\n",
    "    for fold, (train_loader, val_loader) in enumerate(cross_val_loaders):\n",
    "        print(f'FOLD {fold}')\n",
    "        print('--------------------------------')\n",
    "\n",
    "        train_model(model, criterion, optimizer, train_loader, dataset_params, each_model_params, training_params)\n",
    "        validate_model(model, val_loader,dataset_params, each_model_params)\n",
    "        model.__init__(each_model_params, training_params, dataset_params)\n",
    "\n",
    "    print('CrossVaridation Finished')\n",
    "    print('--------------------------------')\n",
    "    #テストデータによる評価\n",
    "    # テストデータローダーの準備\n",
    "\n",
    "#     test_model(model, test_loader)\n",
    "    \n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce2dcf2e-2b2c-4e35-942d-6aa28bacaf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model: nn.Module):\n",
    "    print(\"\\n===== Model Parameters =====\")\n",
    "    for name, param in model.named_parameters():\n",
    "        print(\n",
    "            f\"{name:40s} | shape={tuple(param.shape)} | requires_grad={param.requires_grad}\"\n",
    "        )\n",
    "\n",
    "def print_esn_layer_details(model: ESN):\n",
    "    print(\"\\n===== ESN Layer Details =====\")\n",
    "\n",
    "    # --- Echo State Network ---\n",
    "    esn = model.ESN\n",
    "    print(\"\\n--- EchoStateNetwork ---\")\n",
    "    print(f\"reservoir_size        : {esn.reservoir_size}\")\n",
    "    print(f\"input_size            : {esn.input_size}\")\n",
    "    print(f\"channel_size          : {esn.channel_size}\")\n",
    "    print(f\"sequence_length       : {esn.sequence_length}\")\n",
    "    print(f\"leak_rate             : {esn.leak_rate}\")\n",
    "    print(f\"spectral_radius       : {esn.spectral_radius}\")\n",
    "    print(f\"reservoir_density     : {esn.density}\")\n",
    "    print(f\"input_weights_scale   : {esn.input_weights_scale}\")\n",
    "    print(f\"reservoir_weights_scale: {esn.reservoir_weights_scale}\")\n",
    "\n",
    "    print(\"\\n[Reservoir Weights]\")\n",
    "    print(f\"shape         : {tuple(esn.reservoir_weights.shape)}\")\n",
    "    print(f\"requires_grad : {esn.reservoir_weights.requires_grad}\")\n",
    "\n",
    "    print(\"\\n[Input Weights]\")\n",
    "    print(f\"shape         : {tuple(esn.input_weights.shape)}\")\n",
    "    print(f\"requires_grad : {esn.input_weights.requires_grad}\")\n",
    "\n",
    "    print(\"\\n[Last Reservoir State]\")\n",
    "    print(f\"shape         : {tuple(esn.last_reservoir_state_matrix.shape)}\")\n",
    "\n",
    "    # --- ReadOut ---\n",
    "    readout = model.ReadOut\n",
    "    print(\"\\n--- ReadOut ---\")\n",
    "    print(f\"reservoir_state_matrix_size : {readout.reservoir_state_matrix_size}\")\n",
    "    print(f\"output_size                 : {readout.output_size}\")\n",
    "    print(f\"channel_size                : {readout.channel_size}\")\n",
    "    print(f\"sequence_length             : {readout.sequence_length}\")\n",
    "    print(f\"batch_training              : {readout.batch_training}\")\n",
    "\n",
    "    print(\"\\n[ReadOut Dense Layer]\")\n",
    "    print(f\"weight shape    : {tuple(readout.readout_dense.weight.shape)}\")\n",
    "    print(f\"requires_grad   : {readout.readout_dense.weight.requires_grad}\")\n",
    "    print(f\"bias            : {readout.readout_dense.bias}\")\n",
    "\n",
    "def print_model_structure(model: nn.Module):\n",
    "    print(\"===== Model Structure (Hierarchy) =====\")\n",
    "    for name, module in model.named_modules():\n",
    "        if name == \"\":\n",
    "            continue\n",
    "        print(f\"[{name}] -> {module.__class__.__name__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72266ce6-7201-453e-b4e8-e81ae934663d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Model Structure (Hierarchy) =====\n",
      "[ESN] -> EchoStateNetwork\n",
      "[ReadOut] -> ReadOut\n",
      "[ReadOut.readout_dense] -> Linear\n",
      "\n",
      "===== ESN Layer Details =====\n",
      "\n",
      "--- EchoStateNetwork ---\n",
      "reservoir_size        : 100\n",
      "input_size            : 48\n",
      "channel_size          : 1\n",
      "sequence_length       : 800\n",
      "leak_rate             : 0.7\n",
      "spectral_radius       : 0.9\n",
      "reservoir_density     : 0.02\n",
      "input_weights_scale   : 0.5\n",
      "reservoir_weights_scale: 1.0\n",
      "\n",
      "[Reservoir Weights]\n",
      "shape         : (100, 100)\n",
      "requires_grad : False\n",
      "\n",
      "[Input Weights]\n",
      "shape         : (100, 48)\n",
      "requires_grad : False\n",
      "\n",
      "[Last Reservoir State]\n",
      "shape         : (1, 100)\n",
      "\n",
      "--- ReadOut ---\n",
      "reservoir_state_matrix_size : 100\n",
      "output_size                 : 25\n",
      "channel_size                : 1\n",
      "sequence_length             : 800\n",
      "batch_training              : True\n",
      "\n",
      "[ReadOut Dense Layer]\n",
      "weight shape    : (25, 100)\n",
      "requires_grad   : False\n",
      "bias            : None\n"
     ]
    }
   ],
   "source": [
    "model = ESN(each_model_params, training_params, dataset_params).to(device)\n",
    "print_model_structure(model)\n",
    "print_esn_layer_details(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cab12a5-a241-4a2c-9798-d8d8491d3532",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
