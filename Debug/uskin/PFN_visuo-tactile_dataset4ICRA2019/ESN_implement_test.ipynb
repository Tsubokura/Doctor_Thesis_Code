{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "927c1561-e655-4c49-8445-2debcab2c1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random \n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from scipy import signal\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, random_split, Subset\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "label_encoder = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d2ccad8-4838-4450-a6b5-056b194561c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EchoStateNetwork(nn.Module):\n",
    "    def __init__(self, model_params, dataset_params):\n",
    "        super(EchoStateNetwork, self).__init__()\n",
    "        \n",
    "        self.reservoir_size = int(model_params[\"reservoir_size\"])\n",
    "        self.reservoir_weights_scale = float(model_params[\"reservoir_weights_scale\"])\n",
    "        \n",
    "        self.input_size = int(model_params[\"input_size\"])\n",
    "        self.channel_size = int(model_params[\"channel_size\"])\n",
    "        self.input_weights_scale = float(model_params[\"input_weights_scale\"])\n",
    "        self.spectral_radius = float(model_params[\"spectral_radius\"])\n",
    "        self.density = float(model_params[\"reservoir_density\"])\n",
    "        self.leak_rate = float(model_params[\"leak_rate\"])\n",
    "        \n",
    "        self.sequence_length = int(int(dataset_params[\"sequence_length\"]) / int(dataset_params[\"slicing_size\"]))\n",
    "        # self.sequence_length = int(dataset_params[\"seq_end\"]) - int(dataset_params[\"seq_start\"])\n",
    "\n",
    "        # リザバー結合行列 (ランダムに初期化)\n",
    "        self.register_parameter(\"reservoir_weights\", torch.nn.Parameter(torch.empty((self.reservoir_size, self.reservoir_size)).uniform_(-self.reservoir_weights_scale, self.reservoir_weights_scale).to(device), requires_grad=False))\n",
    "        \n",
    "        # リザバー入力行列 (ランダムに初期化)\n",
    "        self.register_parameter(\"input_weights\", torch.nn.Parameter(torch.empty((self.reservoir_size, self.input_size * self.channel_size)).uniform_(-self.input_weights_scale, self.input_weights_scale).to(device), requires_grad=False))\n",
    "        \n",
    "        \n",
    "\n",
    "        #リザバー結合のスパース処理\n",
    "        self.reservoir_weights_mask = torch.empty((self.reservoir_size, self.reservoir_size)).uniform_(0, 1)\n",
    "        self.reservoir_weights_mask = torch.where(self.reservoir_weights_mask < self.density, torch.tensor(1), torch.tensor(0)).to(device)\n",
    "        self.reservoir_weights *= self.reservoir_weights_mask\n",
    "        \n",
    "        #スペクトル半径の処理\n",
    "        _, singular_values, _ = torch.svd(self.reservoir_weights)\n",
    "        rho_reservoir_weights = torch.max(singular_values).item()\n",
    "        self.reservoir_weights *= self.spectral_radius / rho_reservoir_weights\n",
    "        \n",
    "       #最終時刻における，リザバー状態ベクトル\n",
    "        self.last_reservoir_state_matrix = torch.zeros(self.channel_size, self.reservoir_size).to(device)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        batch_size = x.size(0)\n",
    "        sequence_length = x.size(2)\n",
    "        # x_seq = x.view(batch_size, self.channel_size, self.input_size, sequence_length).to(device)\n",
    "\n",
    "        # if self.last_reservoir_state_matrix is None or self.last_reservoir_state_matrix.size(0) != batch_size:\n",
    "        #     self.last_reservoir_state_matrix = torch.zeros(\n",
    "        #         batch_size, self.channel_size, self.reservoir_size, device=device\n",
    "        #     )\n",
    "        \n",
    "        # 各シーケンスのバッチに対してリザバー状態を初期化\n",
    "        self.reservoir_state_matrix = torch.zeros(batch_size, self.channel_size,  sequence_length, self.reservoir_size).to(device)\n",
    "\n",
    "        self.last_reservoir_state_matrix = torch.zeros(\n",
    "                batch_size, self.channel_size, self.reservoir_size, device=device\n",
    "            )\n",
    "        \n",
    "        for t in range(sequence_length):\n",
    "            input_at_t = torch.matmul(x[:, :, t, :].view(batch_size, -1), self.input_weights.t())\n",
    "            input_at_t = input_at_t.unsqueeze(1)\n",
    "\n",
    "            if t == 0:\n",
    "                state_update = torch.matmul(self.last_reservoir_state_matrix, self.reservoir_weights)\n",
    "            else:\n",
    "                # print(\"11111\")\n",
    "                state_update = torch.matmul(self.reservoir_state_matrix[:, :, t-1, :], self.reservoir_weights)\n",
    "                # print(f\"state_update.shape {state_update.shape}\")\n",
    "            self.reservoir_state_matrix[:, :, t, :] = self.leak_rate * torch.tanh(input_at_t + state_update) + \\\n",
    "                                                    (1 - self.leak_rate) * self.reservoir_state_matrix[:, :, t-1, :]\n",
    "\n",
    "        \n",
    "\n",
    "        self.last_reservoir_state_matrix = self.reservoir_state_matrix[:, :, -1, :]\n",
    "        return self.reservoir_state_matrix\n",
    "\n",
    "    def reset_hidden_state(self):\n",
    "        self.last_reservoir_state_matrix = torch.zeros(self.channel_size, self.reservoir_size, device=device)\n",
    "        # pass\n",
    "        # print(\"内部状態がリセットされました\")\n",
    "    \n",
    "#リードアウト層\n",
    "class ReadOut(nn.Module):\n",
    "    def __init__(self, model_params, dataset_params):\n",
    "        super(ReadOut, self).__init__()\n",
    "        # self.reservoir_state_matrix_size = int(model_params[\"reservoir_size\"]) + int(model_params[\"input_size\"]) + 1\n",
    "        self.reservoir_state_matrix_size = int(model_params[\"reservoir_size\"])\n",
    "        self.output_size = int(model_params[\"ReadOut_output_size\"])\n",
    "        self.batch_training = model_params[\"Batch_Training\"]\n",
    "        self.channel_size = int(model_params[\"channel_size\"])\n",
    "        \n",
    "        self.sequence_length = int(int(dataset_params[\"sequence_length\"]) / int(dataset_params[\"slicing_size\"]))\n",
    "        # self.sequence_length = int(dataset_params[\"seq_end\"]) - int(dataset_params[\"seq_start\"])\n",
    "        \n",
    "        self.readout_dense = nn.Linear(self.reservoir_state_matrix_size, self.output_size, bias=False)\n",
    "        \n",
    "        #線形回帰におけるバッチ学習を行うならば，リードアウト層を最急降下法による学習対象にしない\n",
    "        if self.batch_training == True:\n",
    "            self.readout_dense.weight.requires_grad = False\n",
    "        else:\n",
    "            None\n",
    "            \n",
    "        nn.init.xavier_uniform_(self.readout_dense.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, channel_size, input_size, sequence_length]\n",
    "        # batch_size, channel_size, input_size, sequence_length = x.size()\n",
    "        \n",
    "        # Reshape x to apply the dense layer to each time step and channel independently\n",
    "        # New shape: [batch_size * channel_size * sequence_length, input_size]\n",
    "        # x_reshaped = x.permute(0, 1, 3, 2).contiguous().view(-1, input_size)\n",
    "        \n",
    "        # Apply the dense layer\n",
    "        # Output shape: [batch_size * channel_size * sequence_length, output_size]\n",
    "        # print(f\"readout x shape{x.shape}\")\n",
    "        output = self.readout_dense(x)\n",
    "        \n",
    "        # Reshape the output back to the original form\n",
    "        # Output shape: [batch_size, channel_size, output_size, sequence_length]\n",
    "        # output = output.view(batch_size, channel_size, sequence_length, -1).permute(0, 1, 3, 2).contiguous()\n",
    "        return output\n",
    "    \n",
    "    # リッジ回帰によるリードアウトの導出\n",
    "    @staticmethod\n",
    "    def ridge_regression(X, Y, alpha):\n",
    "        # データ行列 X の形状を取得\n",
    "        n, p = X.shape\n",
    "\n",
    "        # 正則化項の行列を作成\n",
    "        ridge_matrix = (alpha * torch.eye(n)).float().to(device)\n",
    "        X = X.float()\n",
    "        Y = Y.float()\n",
    "\n",
    "        # リッジ回帰の係数を計算\n",
    "        coefficients = torch.linalg.solve(torch.matmul(X, X.T) + ridge_matrix, torch.matmul(X, Y.T)).T\n",
    "\n",
    "        return coefficients\n",
    "\n",
    "    @staticmethod\n",
    "    def ridge_regression_update(outputs, targets, model, alpha=0):\n",
    "        with torch.no_grad():\n",
    "            # リッジ回帰を用いて重みを求める\n",
    "            # モデルのパラメータを更新\n",
    "            new_weights = ReadOut.ridge_regression(outputs.squeeze(), targets.squeeze(), alpha)\n",
    "\n",
    "            # モデルのパラメータを更新\n",
    "            model.ReadOut.readout_dense.weight.copy_(new_weights)\n",
    "        \n",
    "        return None\n",
    "    #それぞれのモデルパラメータ候補を辞書に格納する\n",
    "    @staticmethod\n",
    "    def model_params_candinate(model_params):\n",
    "        model_params_combinations = list(itertools.product(*model_params.values()))\n",
    "        param_dicts = [dict(zip(model_params.keys(), combination)) for combination in model_params_combinations]\n",
    "        \n",
    "        return param_dicts\n",
    "\n",
    "    #モデル構造を辞書型に格納\n",
    "    @staticmethod\n",
    "    def model_sturcture_dict(model):\n",
    "        layers_dict = {}\n",
    "        for name, module in model.named_modules():\n",
    "            layers_dict[name] = {\n",
    "                'type': type(module).__name__,\n",
    "                'parameters': {p: getattr(module, p) for p in module.__dict__ if not p.startswith('_')}\n",
    "            }\n",
    "        \n",
    "        #モデル名と初期の引数は削除\n",
    "        del(layers_dict[''])\n",
    "        \n",
    "        return layers_dict\n",
    "\n",
    "class ESN(nn.Module):\n",
    "    def __init__(self, model_params, training_params, dataset_params):\n",
    "        super(ESN, self).__init__()\n",
    "        self.ESN = EchoStateNetwork(model_params, dataset_params)\n",
    "        self.ReadOut = ReadOut(model_params, dataset_params)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.Reservoir_State_Matrix = self.ESN(x)\n",
    "        # print(f\"resever state matrix {self.Reservoir_State_Matrix.shape}\")\n",
    "        self.ReadOut_Reservoir = self.ReadOut(self.Reservoir_State_Matrix)\n",
    "        # print(\"重み:\", self.ReadOut.readout_dense.weight)\n",
    "        # print(self.ReadOut_Reservoir)\n",
    "\n",
    "        #channle_size次元はいらないので，減らす\n",
    "        self.ReadOut_Reservoir = self.ReadOut_Reservoir.squeeze(1)\n",
    "        # print(f\"output matrix {self.ReadOut_Reservoir.shape}\")\n",
    "\n",
    "        return self.ReadOut_Reservoir\n",
    "\n",
    "    def reset_hidden_state(self):\n",
    "        self.ESN.reset_hidden_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35ab0bee-9bd7-448d-83de-765e80428d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0],\n",
      "          [ 1],\n",
      "          [ 2],\n",
      "          [ 3],\n",
      "          [ 4],\n",
      "          [ 5],\n",
      "          [ 6],\n",
      "          [ 7],\n",
      "          [ 8],\n",
      "          [ 9],\n",
      "          [10],\n",
      "          [11]]]])\n",
      "tensor([[[[ 0.,  0.,  0.,  0.],\n",
      "          [ 1.,  0.,  0.,  0.],\n",
      "          [ 2.,  0.,  0.,  0.],\n",
      "          [ 3.,  0.,  0.,  0.],\n",
      "          [ 4.,  0.,  0.,  0.],\n",
      "          [ 5.,  0.,  0.,  0.],\n",
      "          [ 6.,  0.,  0.,  0.],\n",
      "          [ 7.,  0.,  0.,  0.],\n",
      "          [ 8.,  0.,  0.,  0.],\n",
      "          [ 9.,  0.,  0.,  0.],\n",
      "          [10.,  0.,  0.,  0.],\n",
      "          [11.,  0.,  0.,  0.]]],\n",
      "\n",
      "\n",
      "        [[[ 0.,  0.,  0.,  0.],\n",
      "          [ 1.,  0.,  0.,  0.],\n",
      "          [ 2.,  0.,  0.,  0.],\n",
      "          [ 3.,  0.,  0.,  0.],\n",
      "          [ 4.,  0.,  0.,  0.],\n",
      "          [ 5.,  0.,  0.,  0.],\n",
      "          [ 6.,  0.,  0.,  0.],\n",
      "          [ 7.,  0.,  0.,  0.],\n",
      "          [ 8.,  0.,  0.,  0.],\n",
      "          [ 9.,  0.,  0.,  0.],\n",
      "          [10.,  0.,  0.,  0.],\n",
      "          [11.,  0.,  0.,  0.]]]])\n",
      "torch.Size([2, 1, 12, 100])\n",
      "torch.Size([2, 12, 25])\n"
     ]
    }
   ],
   "source": [
    "def make_time_coded_x(B, C, T, F, device):\n",
    "    x = torch.zeros(B, C, T, F, device=device)\n",
    "    t = torch.arange(T, device=device).view(1,1,T,1)\n",
    "    print(t)\n",
    "    x[..., 0:1] = t  # feature0に時刻を埋め込む\n",
    "    print(x)\n",
    "    return x\n",
    "\n",
    "@torch.no_grad()\n",
    "def debug_time_axis(model, B=2, C=1, T=12, F=4):\n",
    "    x = make_time_coded_x(B,C,T,F,device)\n",
    "    # forward前の確認\n",
    "    for t in range(T):\n",
    "        assert torch.all(x[:, :, t, 0] == t), \"入力のtime軸が想定と違う\"\n",
    "    y = model.ESN(x)   # Reservoir states: (B,C,T,R)\n",
    "    y_out = model(x)   # Reservoir states: (B,C,T,R)\n",
    "    print(y.shape)\n",
    "    print(y_out.shape)\n",
    "    assert y.shape[2] == T, \"Reservoir側でtime次元が崩れている\"\n",
    "\n",
    "dataset_params = {\"seq_start\" : 400, \"seq_end\" : 1200, \"sequence_length\": 800, \"slicing_size\" : 1, \"augmentation_factor\": 0, \"batch_size\" : 32, \"Onehot_Encoding\" : None, \"augmentation_mu\" : 0, \"augmentation_sigma\" : 0, \"augmentation_shift\" : 1, \"time_stride\":1}\n",
    "model_params = {\"reservoir_size\" : 100,\"input_size\" : 4, \"channel_size\" : 1,  \"reservoir_weights_scale\" : 1, \"input_weights_scale\" : 10000, \"spectral_radius\" : 1.5,\"reservoir_density\" : 0.02, \"leak_rate\" : 0.1, \"Batch_Training\" : True, \"ReadOut_output_size\" : 25, \"Regularization_L2\" : 0.01}\n",
    "training_params = {\"num_epochs\" : 1, \"learning_rate\" : 0.01, \"weight_decay\" : 1e-2, \"testdata_ratio\" : 0, \"n_splits\" : 5}\n",
    "\n",
    "model = ESN(model_params, training_params, dataset_params).to(device)\n",
    "debug_time_axis(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266f37bf-92ef-483b-9900-0a092939860c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
